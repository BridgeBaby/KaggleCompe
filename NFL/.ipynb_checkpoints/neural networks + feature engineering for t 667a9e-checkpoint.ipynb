{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "a9798604-b0ed-4ea9-b894-52b8fad97b7b",
    "_uuid": "ff2644fd-b910-43e5-9a2f-959c0adeb15a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "#from kaggle.competitions import nflrush\n",
    "import tqdm\n",
    "import re\n",
    "from string import punctuation\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import keras\n",
    "from keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, EarlyStopping\n",
    "from keras.utils import plot_model\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "sns.set_style('darkgrid')\n",
    "mpl.rcParams['figure.figsize'] = [15,10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "1fbf87f8-5122-4daf-bc70-3e8d31bb5856",
    "_uuid": "fb699687-2bf2-471a-845e-d498aeee7eac"
   },
   "outputs": [],
   "source": [
    "#env = nflrush.make_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "4f97b8fd-2dac-4fe0-aa0b-8c1d1cc7d66f",
    "_uuid": "72f9ce60-48f7-4666-80a4-c343e94bcc5a"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'../input/nfl-big-data-bowl-2020/train.csv' does not exist: b'../input/nfl-big-data-bowl-2020/train.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-dda2188934c6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'../input/nfl-big-data-bowl-2020/train.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'WindSpeed'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'object'\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\tf-20-gpu\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    683\u001b[0m         )\n\u001b[0;32m    684\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 685\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    686\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    687\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\tf-20-gpu\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 457\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    458\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\tf-20-gpu\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 895\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    896\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\tf-20-gpu\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1133\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1135\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1136\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1137\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\tf-20-gpu\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1904\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"usecols\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1905\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1906\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1907\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1908\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] File b'../input/nfl-big-data-bowl-2020/train.csv' does not exist: b'../input/nfl-big-data-bowl-2020/train.csv'"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('../input/nfl-big-data-bowl-2020/train.csv', dtype={'WindSpeed': 'object'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "89c007b1-c569-4323-b5c9-e7e5b3357fa5",
    "_uuid": "b1676ce2-3fa9-4417-bdcc-c44fa6ecbde1"
   },
   "source": [
    "# Overall analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "f352f6e7-a676-424f-99a6-667d329b75cc",
    "_uuid": "77f26cc6-4a5a-42ef-9871-8fe4d2b204af"
   },
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from https://www.kaggle.com/prashantkikani/nfl-starter-lgb-feature-engg\n",
    "train['DefendersInTheBox_vs_Distance'] = train['DefendersInTheBox'] / train['Distance']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "bbf962b7-28fe-4b1d-8ce7-af815b5ffe9d",
    "_uuid": "9dd3edf6-c372-4e94-a018-b9fdbc13996a"
   },
   "source": [
    "# Categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "2996d6a4-18fc-4ce8-a2d0-a0a58b2b2391",
    "_uuid": "a4f3359c-ab23-413e-b87a-5e9bfa406be6"
   },
   "outputs": [],
   "source": [
    "cat_features = []\n",
    "for col in train.columns:\n",
    "    if train[col].dtype =='object':\n",
    "        cat_features.append((col, len(train[col].unique())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "f07d9476-d360-4f37-9118-5954fb6b591d",
    "_uuid": "1c43c14a-5a9c-4245-8510-285e435ae314"
   },
   "source": [
    "Let's preprocess some of those features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stadium Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['StadiumType'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already can see some typos, let's fix them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_StadiumType(txt):\n",
    "    if pd.isna(txt):\n",
    "        return np.nan\n",
    "    txt = txt.lower()\n",
    "    txt = ''.join([c for c in txt if c not in punctuation])\n",
    "    txt = re.sub(' +', ' ', txt)\n",
    "    txt = txt.strip()\n",
    "    txt = txt.replace('outside', 'outdoor')\n",
    "    txt = txt.replace('outdor', 'outdoor')\n",
    "    txt = txt.replace('outddors', 'outdoor')\n",
    "    txt = txt.replace('outdoors', 'outdoor')\n",
    "    txt = txt.replace('oudoor', 'outdoor')\n",
    "    txt = txt.replace('indoors', 'indoor')\n",
    "    txt = txt.replace('ourdoor', 'outdoor')\n",
    "    txt = txt.replace('retractable', 'rtr.')\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['StadiumType'] = train['StadiumType'].apply(clean_StadiumType)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By pareto's principle we are just going to focus on the words: outdoor, indoor, closed and open."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_StadiumType(txt):\n",
    "    if pd.isna(txt):\n",
    "        return np.nan\n",
    "    if 'outdoor' in txt or 'open' in txt:\n",
    "        return 1\n",
    "    if 'indoor' in txt or 'closed' in txt:\n",
    "        return 0\n",
    "    \n",
    "    return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['StadiumType'] = train['StadiumType'].apply(transform_StadiumType)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Turf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from https://www.kaggle.com/c/nfl-big-data-bowl-2020/discussion/112681#latest-649087\n",
    "Turf = {'Field Turf':'Artificial', 'A-Turf Titan':'Artificial', 'Grass':'Natural', 'UBU Sports Speed S5-M':'Artificial', \n",
    "        'Artificial':'Artificial', 'DD GrassMaster':'Artificial', 'Natural Grass':'Natural', \n",
    "        'UBU Speed Series-S5-M':'Artificial', 'FieldTurf':'Artificial', 'FieldTurf 360':'Artificial', 'Natural grass':'Natural', 'grass':'Natural', \n",
    "        'Natural':'Natural', 'Artifical':'Artificial', 'FieldTurf360':'Artificial', 'Naturall Grass':'Natural', 'Field turf':'Artificial', \n",
    "        'SISGrass':'Artificial', 'Twenty-Four/Seven Turf':'Artificial', 'natural grass':'Natural'} \n",
    "\n",
    "train['Turf'] = train['Turf'].map(Turf)\n",
    "train['Turf'] = train['Turf'] == 'Natural'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Possession Team"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[(train['PossessionTeam']!=train['HomeTeamAbbr']) & (train['PossessionTeam']!=train['VisitorTeamAbbr'])][['PossessionTeam', 'HomeTeamAbbr', 'VisitorTeamAbbr']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have some problem with the enconding of the teams such as BLT and BAL or ARZ and ARI.\n",
    "\n",
    "Let's try to fix them manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(train['HomeTeamAbbr'].unique()) == sorted(train['VisitorTeamAbbr'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_abbr = []\n",
    "for x,y  in zip(sorted(train['HomeTeamAbbr'].unique()), sorted(train['PossessionTeam'].unique())):\n",
    "    if x!=y:\n",
    "        print(x + \" \" + y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apparently these are the only three problems, let's fix it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_abbr = {'ARI': 'ARZ', 'BAL': 'BLT', 'CLE': 'CLV', 'HOU': 'HST'}\n",
    "for abb in train['PossessionTeam'].unique():\n",
    "    map_abbr[abb] = abb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['PossessionTeam'] = train['PossessionTeam'].map(map_abbr)\n",
    "train['HomeTeamAbbr'] = train['HomeTeamAbbr'].map(map_abbr)\n",
    "train['VisitorTeamAbbr'] = train['VisitorTeamAbbr'].map(map_abbr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['HomePossesion'] = train['PossessionTeam'] == train['HomeTeamAbbr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Field_eq_Possession'] = train['FieldPosition'] == train['PossessionTeam']\n",
    "train['HomeField'] = train['FieldPosition'] == train['HomeTeamAbbr']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Offense formation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "off_form = train['OffenseFormation'].unique()\n",
    "train['OffenseFormation'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since I don't have any knowledge about formations, I am just goig to one-hot encode this feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.concat([train.drop(['OffenseFormation'], axis=1), pd.get_dummies(train['OffenseFormation'], prefix='Formation')], axis=1)\n",
    "dummy_col = train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "d7f81adc-1a29-492d-ba29-b64c98485431",
    "_uuid": "bb13b517-32ff-4cdd-93f3-31b3b79dbbc5"
   },
   "source": [
    "## Game Clock"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b19b1526-cf9f-49f6-b462-b0a771b9209a",
    "_uuid": "8e4d32ec-3096-4d7c-8be4-2bb93e2d7a3b"
   },
   "source": [
    "Game clock is supposed to be a numerical feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "416aa3ed-50f2-422c-9b0d-07e9b83460d7",
    "_uuid": "3009010a-053c-46a6-b442-d20b20b20eb3"
   },
   "outputs": [],
   "source": [
    "train['GameClock'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "e5250758-ca1a-4e4b-927e-9e9de65fbe0d",
    "_uuid": "5cb3fab2-337c-4edd-a540-2dc5574785d4"
   },
   "source": [
    "Since we already have the quarter feature, we can just divide the Game Clock by 15 minutes so we can get the normalized time left in the quarter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "1d691a4f-b74c-49fa-b269-e129b0a95173",
    "_uuid": "d67573bc-dc38-42fc-9f30-a5e397206e0f"
   },
   "outputs": [],
   "source": [
    "def strtoseconds(txt):\n",
    "    txt = txt.split(':')\n",
    "    ans = int(txt[0])*60 + int(txt[1]) + int(txt[2])/60\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "af428630-c6ff-4e4f-a293-c18a11738d15",
    "_uuid": "a8cdc425-6d86-4be2-8d73-d368d94b8231"
   },
   "outputs": [],
   "source": [
    "train['GameClock'] = train['GameClock'].apply(strtoseconds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "99fb1a0a-6688-4eb1-8754-f82454775876",
    "_uuid": "363e6275-6ad9-4c89-af4c-2c435a7d5d1c"
   },
   "outputs": [],
   "source": [
    "sns.distplot(train['GameClock'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "f58a2052-0669-449f-8e82-4c1be7ef9e8a",
    "_uuid": "0cc3cefd-a51c-482e-b921-c15940d4cbc3"
   },
   "source": [
    "## Player height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "fd1c0d2e-e465-46fe-84e5-7943cff06e4a",
    "_uuid": "32568e2c-dd0f-44e2-a516-5a8a9fc82cd9"
   },
   "outputs": [],
   "source": [
    "train['PlayerHeight']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "86749dbf-0889-4167-9b5a-640601da94b8",
    "_uuid": "98e1a5b4-5329-4966-aae4-ab203617b352"
   },
   "source": [
    "We know that 1ft=12in, thus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "0f0f6f7c-fb7b-49df-aa9d-32108abeb911",
    "_uuid": "00e6126d-2c75-4b0d-9a39-ffa00799cd48"
   },
   "outputs": [],
   "source": [
    "train['PlayerHeight'] = train['PlayerHeight'].apply(lambda x: 12*int(x.split('-')[0])+int(x.split('-')[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['PlayerBMI'] = 703*(train['PlayerWeight']/(train['PlayerHeight'])**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "ca1fd3b8-911c-4c95-9a75-a29336960281",
    "_uuid": "4731392d-85b3-4f06-ad10-d2e855259404"
   },
   "source": [
    "## Time handoff and snap and Player BirthDate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "6303a87a-570f-40c5-a368-5727515d085f",
    "_uuid": "1ee8517c-ad58-4dd0-94a5-13a9d3657393"
   },
   "outputs": [],
   "source": [
    "train['TimeHandoff']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "c4925fa7-aa2d-45dc-9c67-73e7465852c9",
    "_uuid": "3f9ded81-1d2a-4c36-84dd-efcb60f184bc"
   },
   "outputs": [],
   "source": [
    "train['TimeHandoff'] = train['TimeHandoff'].apply(lambda x: datetime.datetime.strptime(x, \"%Y-%m-%dT%H:%M:%S.%fZ\"))\n",
    "train['TimeSnap'] = train['TimeSnap'].apply(lambda x: datetime.datetime.strptime(x, \"%Y-%m-%dT%H:%M:%S.%fZ\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "c0abdaa4-da54-463b-8d14-9791f7e58c21",
    "_uuid": "3ac57217-a252-42ec-b067-cefa2b5fd72b"
   },
   "outputs": [],
   "source": [
    "train['TimeDelta'] = train.apply(lambda row: (row['TimeHandoff'] - row['TimeSnap']).total_seconds(), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "74726374-d122-472d-ab55-3bdec8169a3c",
    "_uuid": "1469462e-2416-4b55-a62f-0b7f4016d633"
   },
   "outputs": [],
   "source": [
    "train['PlayerBirthDate'] = train['PlayerBirthDate'].apply(lambda x: datetime.datetime.strptime(x, \"%m/%d/%Y\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "d35c0fc1-af5b-4cfa-bf94-cfb0b455605b",
    "_uuid": "8f6c0a58-2fd9-4da4-a775-935d30f01870"
   },
   "source": [
    "Let's use the time handoff to calculate the players age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "df4b2c6a-3b1d-4434-acea-b7b5281e162c",
    "_uuid": "81518246-4fd1-4a17-aa5f-4b6fe58de355"
   },
   "outputs": [],
   "source": [
    "seconds_in_year = 60*60*24*365.25\n",
    "train['PlayerAge'] = train.apply(lambda row: (row['TimeHandoff']-row['PlayerBirthDate']).total_seconds()/seconds_in_year, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "4b49a9d6-7ebb-4b9d-8574-7e547c3db476",
    "_uuid": "901347f8-8e6b-43a2-aa0e-545df16da154"
   },
   "outputs": [],
   "source": [
    "train = train.drop(['TimeHandoff', 'TimeSnap', 'PlayerBirthDate'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "a53e8cdd-fe2b-45de-89f8-0c443d7bc99d",
    "_uuid": "9e964c7d-fabd-49f2-afe9-1629d9753a86"
   },
   "source": [
    "## Wind Speed and Direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "6a6cb596-038c-4d87-85af-bfbf1fcfe490",
    "_uuid": "027714f6-7b58-4476-b825-c5fc0c7490cc"
   },
   "outputs": [],
   "source": [
    "train['WindSpeed'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "e84b4ef8-137b-43e1-b8d1-59af7df321c4",
    "_uuid": "c9011df9-0fad-43e0-b6ae-d424eebac52b"
   },
   "source": [
    "We can see there are some values that are not standardized(e.g. 12mph), we are going to remove mph from all our values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "c15a9234-d9bc-42c2-96f1-8cfdc4ff330e",
    "_uuid": "2176aa4b-3acc-4487-bb42-e0f0f1f9d73f"
   },
   "outputs": [],
   "source": [
    "train['WindSpeed'] = train['WindSpeed'].apply(lambda x: x.lower().replace('mph', '').strip() if not pd.isna(x) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b837d9a5-d033-4701-ba15-f57acbce9af0",
    "_uuid": "ab6dcae3-76ca-4e05-8b52-43213d566f1a"
   },
   "outputs": [],
   "source": [
    "train['WindSpeed'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "eca7f782-002a-40f6-9ed9-d63a91f4f5f6",
    "_uuid": "5f3aade2-e103-4d79-aaaf-3491a08dc3e7"
   },
   "outputs": [],
   "source": [
    "#let's replace the ones that has x-y by (x+y)/2\n",
    "# and also the ones with x gusts up to y\n",
    "train['WindSpeed'] = train['WindSpeed'].apply(lambda x: (int(x.split('-')[0])+int(x.split('-')[1]))/2 if not pd.isna(x) and '-' in x else x)\n",
    "train['WindSpeed'] = train['WindSpeed'].apply(lambda x: (int(x.split()[0])+int(x.split()[-1]))/2 if not pd.isna(x) and type(x)!=float and 'gusts up to' in x else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "2599fa48-430f-4fe8-a4f2-22db4439c87a",
    "_uuid": "0a40d464-1af1-4413-b967-cb600c39c289"
   },
   "outputs": [],
   "source": [
    "def str_to_float(txt):\n",
    "    try:\n",
    "        return float(txt)\n",
    "    except:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "98329992-3f20-439c-b7da-4e5edde2c7f3",
    "_uuid": "765da928-abe6-4c84-b998-f09531cf4f1e"
   },
   "outputs": [],
   "source": [
    "train['WindSpeed'] = train['WindSpeed'].apply(str_to_float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "10ef0c46-fa71-4131-ae52-3dc8402e4208",
    "_uuid": "41c59356-856f-43c8-9b47-fe469c256eb5",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train['WindDirection'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_WindDirection(txt):\n",
    "    if pd.isna(txt):\n",
    "        return np.nan\n",
    "    txt = txt.lower()\n",
    "    txt = ''.join([c for c in txt if c not in punctuation])\n",
    "    txt = txt.replace('from', '')\n",
    "    txt = txt.replace(' ', '')\n",
    "    txt = txt.replace('north', 'n')\n",
    "    txt = txt.replace('south', 's')\n",
    "    txt = txt.replace('west', 'w')\n",
    "    txt = txt.replace('east', 'e')\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['WindDirection'] = train['WindDirection'].apply(clean_WindDirection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['WindDirection'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_WindDirection(txt):\n",
    "    if pd.isna(txt):\n",
    "        return np.nan\n",
    "    \n",
    "    if txt=='n':\n",
    "        return 0\n",
    "    if txt=='nne' or txt=='nen':\n",
    "        return 1/8\n",
    "    if txt=='ne':\n",
    "        return 2/8\n",
    "    if txt=='ene' or txt=='nee':\n",
    "        return 3/8\n",
    "    if txt=='e':\n",
    "        return 4/8\n",
    "    if txt=='ese' or txt=='see':\n",
    "        return 5/8\n",
    "    if txt=='se':\n",
    "        return 6/8\n",
    "    if txt=='ses' or txt=='sse':\n",
    "        return 7/8\n",
    "    if txt=='s':\n",
    "        return 8/8\n",
    "    if txt=='ssw' or txt=='sws':\n",
    "        return 9/8\n",
    "    if txt=='sw':\n",
    "        return 10/8\n",
    "    if txt=='sww' or txt=='wsw':\n",
    "        return 11/8\n",
    "    if txt=='w':\n",
    "        return 12/8\n",
    "    if txt=='wnw' or txt=='nww':\n",
    "        return 13/8\n",
    "    if txt=='nw':\n",
    "        return 14/8\n",
    "    if txt=='nwn' or txt=='nnw':\n",
    "        return 15/8\n",
    "    return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['WindDirection'] = train['WindDirection'].apply(transform_WindDirection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "5bcc44c6-9495-4d62-8551-dd88ce4f4de2",
    "_uuid": "4dfb519d-24fa-4bf7-a9c7-a59b285caa4a"
   },
   "source": [
    "## PlayDirection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "97451668-f804-4c81-b8b9-15f2be5614c2",
    "_uuid": "5b93e997-86ec-4227-8437-b8889d6e78ed"
   },
   "outputs": [],
   "source": [
    "train['PlayDirection'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b04eae61-9135-4fbf-a331-f39c7df71656",
    "_uuid": "6f987647-ccf7-4cb3-b8a0-2701e4488052"
   },
   "outputs": [],
   "source": [
    "train['PlayDirection'] = train['PlayDirection'].apply(lambda x: x.strip() == 'right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "e23f8f1c-5a59-4354-afd9-1852aabb148a",
    "_uuid": "2c0a1fd0-4e2c-4bf0-8c9b-95bc6e20c1d7"
   },
   "source": [
    "## Team"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "29f59ee0-aa87-4552-812f-488eac6647ef",
    "_uuid": "753a80be-f9f4-4b3e-9aff-6c0226302583"
   },
   "outputs": [],
   "source": [
    "train['Team'] = train['Team'].apply(lambda x: x.strip()=='home')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "9fd464a2-9e74-4682-a2a7-10481313f476",
    "_uuid": "c5dc1be2-77b4-4c09-8bb5-ea1052603e10"
   },
   "source": [
    "## Game Weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "77190f8e-9f1d-43df-99b0-bebd20936106",
    "_uuid": "19cb48f4-e188-4eff-8b44-5f3651e3a301"
   },
   "outputs": [],
   "source": [
    "train['GameWeather'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "75ba864e-c328-4929-8090-4310ec34397f",
    "_uuid": "c2eb5e45-9f3d-45ec-9312-136db8918163"
   },
   "source": [
    "We are going to apply the following preprocessing:\n",
    " \n",
    "- Lower case\n",
    "- N/A Indoor, N/A (Indoors) and Indoor => indoor Let's try to cluster those together.\n",
    "- coudy and clouidy => cloudy\n",
    "- party => partly\n",
    "- sunny and clear => clear and sunny\n",
    "- skies and mostly => \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "7ad94b8c-5414-48b4-a495-cb3a2ba13522",
    "_uuid": "68de9ad0-dbb7-4cbc-b40d-7b81d55222e7"
   },
   "outputs": [],
   "source": [
    "train['GameWeather'] = train['GameWeather'].str.lower()\n",
    "indoor = \"indoor\"\n",
    "train['GameWeather'] = train['GameWeather'].apply(lambda x: indoor if not pd.isna(x) and indoor in x else x)\n",
    "train['GameWeather'] = train['GameWeather'].apply(lambda x: x.replace('coudy', 'cloudy').replace('clouidy', 'cloudy').replace('party', 'partly') if not pd.isna(x) else x)\n",
    "train['GameWeather'] = train['GameWeather'].apply(lambda x: x.replace('clear and sunny', 'sunny and clear') if not pd.isna(x) else x)\n",
    "train['GameWeather'] = train['GameWeather'].apply(lambda x: x.replace('skies', '').replace(\"mostly\", \"\").strip() if not pd.isna(x) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "36bcdbfb-a2eb-4980-876f-efa5ffa14409",
    "_uuid": "10cea94f-b2b5-4206-850d-07dbdc22abb0"
   },
   "outputs": [],
   "source": [
    "train['GameWeather'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "c78e1f1c-d73f-4a2e-989f-bb02c180ca0f",
    "_uuid": "ed2134d8-fa03-4264-a801-00b7bc66ddef"
   },
   "source": [
    "Let's now look at the most common words we have in the weather description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "61ddf642-4584-4147-931c-13ae480cee0d",
    "_uuid": "2ac09166-d193-4758-8b39-4ea87f971b89"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "weather_count = Counter()\n",
    "for weather in train['GameWeather']:\n",
    "    if pd.isna(weather):\n",
    "        continue\n",
    "    for word in weather.split():\n",
    "        weather_count[word]+=1\n",
    "        \n",
    "weather_count.most_common()[:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "2cc3150c-51c5-4668-b221-a13d6a041cd1",
    "_uuid": "bbcb19d3-3003-433e-b7d1-fc444cd7c431"
   },
   "source": [
    "To encode our weather we are going to do the following map:\n",
    " \n",
    "- climate controlled or indoor => 3, sunny or sun => 2, clear => 1, cloudy => -1, rain => -2, snow => -3, others => 0\n",
    "- partly => multiply by 0.5\n",
    "\n",
    "I don't have any expercience with american football so I don't know if playing in a climate controlled or indoor stadium is good or not, if someone has a good idea on how to encode this it would be nice to leave it in the comments :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "8756e015-060c-45f2-a6c8-4a81ac9de59f",
    "_uuid": "15d456c6-3600-4468-9893-e1482238a813"
   },
   "outputs": [],
   "source": [
    "def map_weather(txt):\n",
    "    ans = 1\n",
    "    if pd.isna(txt):\n",
    "        return 0\n",
    "    if 'partly' in txt:\n",
    "        ans*=0.5\n",
    "    if 'climate controlled' in txt or 'indoor' in txt:\n",
    "        return ans*3\n",
    "    if 'sunny' in txt or 'sun' in txt:\n",
    "        return ans*2\n",
    "    if 'clear' in txt:\n",
    "        return ans\n",
    "    if 'cloudy' in txt:\n",
    "        return -ans\n",
    "    if 'rain' in txt or 'rainy' in txt:\n",
    "        return -2*ans\n",
    "    if 'snow' in txt:\n",
    "        return -3*ans\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "ff5f7d03-205e-4935-b22b-962382004e5b",
    "_uuid": "f26ae854-d725-4242-a753-209e7664f032"
   },
   "outputs": [],
   "source": [
    "train['GameWeather'] = train['GameWeather'].apply(map_weather)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "8692b4e2-3846-43a1-b4a4-8c4333cc2ea1",
    "_uuid": "b8a66d46-4635-4a89-a2c6-ec65e1bddfb2"
   },
   "source": [
    "## NflId NflIdRusher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "6276a197-f31c-4bf5-8663-397fc89b51b0",
    "_uuid": "df832e5b-3d72-4763-9d3d-051ab24f8cd5"
   },
   "outputs": [],
   "source": [
    "train['IsRusher'] = train['NflId'] == train['NflIdRusher']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "99320871-af57-4927-b38f-4df49e163082",
    "_uuid": "3de6f253-7a26-4364-8ee1-b2585278bc64"
   },
   "outputs": [],
   "source": [
    "train.drop(['NflId', 'NflIdRusher'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PlayDirection problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, we have a problem if some features such as X and Y because of the play direction, let's fix those issues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### X, orientation and direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['X'] = train.apply(lambda row: row['X'] if row['PlayDirection'] else 120-row['X'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from https://www.kaggle.com/scirpus/hybrid-gp-and-nn\n",
    "def new_orientation(angle, play_direction):\n",
    "    if play_direction == 0:\n",
    "        new_angle = 360.0 - angle\n",
    "        if new_angle == 360.0:\n",
    "            new_angle = 0.0\n",
    "        return new_angle\n",
    "    else:\n",
    "        return angle\n",
    "    \n",
    "train['Orientation'] = train.apply(lambda row: new_orientation(row['Orientation'], row['PlayDirection']), axis=1)\n",
    "train['Dir'] = train.apply(lambda row: new_orientation(row['Dir'], row['PlayDirection']), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## YardsLeft\n",
    "\n",
    "Let's compute how many yards are left to the end-zone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['YardsLeft'] = train.apply(lambda row: 100-row['YardLine'] if row['HomeField'] else row['YardLine'], axis=1)\n",
    "train['YardsLeft'] = train.apply(lambda row: row['YardsLeft'] if row['PlayDirection'] else 100-row['YardsLeft'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "((train['YardsLeft']<train['Yards']) | (train['YardsLeft']-100>train['Yards'])).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly:\n",
    "Yards<=YardsLeft and YardsLeft-100<=Yards, thus we are going to drop those wrong lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.drop(train.index[(train['YardsLeft']<train['Yards']) | (train['YardsLeft']-100>train['Yards'])], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "280b0090-ad48-4c21-91fb-4f762159b74e",
    "_uuid": "14db5230-0ae7-448c-b595-b9683230fe91"
   },
   "source": [
    "# Baseline model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "277c1578-7565-4c59-bca9-c7830f203a09",
    "_uuid": "e075f454-f0e4-4913-bc36-b8f1425f3a58"
   },
   "source": [
    "Let's drop the categorical features and run a simple random forest in our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.sort_values(by=['PlayId', 'Team', 'IsRusher', 'JerseyNumber']).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.drop(['GameId', 'PlayId', 'index', 'IsRusher', 'Team'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "27659cfa-4c5a-42a5-93bf-2ccefd5aff09",
    "_uuid": "d93d3612-a452-46d0-a796-ed8813158517"
   },
   "outputs": [],
   "source": [
    "cat_features = []\n",
    "for col in train.columns:\n",
    "    if train[col].dtype =='object':\n",
    "        cat_features.append(col)\n",
    "        \n",
    "train = train.drop(cat_features, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "94a6f68f-b249-41cb-92c0-0b9ffd2fe60b",
    "_uuid": "742c41b8-26fa-4f36-b8a1-9dd7d2b53d52"
   },
   "source": [
    "We are now going to make one big row for each play where the rusher is the last one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.fillna(-999, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "players_col = []\n",
    "for col in train.columns:\n",
    "    if train[col][:22].std()!=0:\n",
    "        players_col.append(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(train[players_col]).reshape(-1, len(players_col)*22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "play_col = train.drop(players_col+['Yards'], axis=1).columns\n",
    "X_play_col = np.zeros(shape=(X_train.shape[0], len(play_col)))\n",
    "for i, col in enumerate(play_col):\n",
    "    X_play_col[:, i] = train[col][::22]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.concatenate([X_train, X_play_col], axis=1)\n",
    "y_train = np.zeros(shape=(X_train.shape[0], 199))\n",
    "for i,yard in enumerate(train['Yards'][::22]):\n",
    "    y_train[i, yard+99:] = np.ones(shape=(1, 100-yard))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAdam(keras.optimizers.Optimizer):\n",
    "    \"\"\"RAdam optimizer.\n",
    "    # Arguments\n",
    "        learning_rate: float >= 0. Learning rate.\n",
    "        beta_1: float, 0 < beta < 1. Generally close to 1.\n",
    "        beta_2: float, 0 < beta < 1. Generally close to 1.\n",
    "        epsilon: float >= 0. Fuzz factor. If `None`, defaults to `K.epsilon()`.\n",
    "        decay: float >= 0. Learning rate decay over each update.\n",
    "        weight_decay: float >= 0. Weight decay for each param.\n",
    "        amsgrad: boolean. Whether to apply the AMSGrad variant of this\n",
    "            algorithm from the paper \"On the Convergence of Adam and\n",
    "            Beyond\".\n",
    "        total_steps: int >= 0. Total number of training steps. Enable warmup by setting a positive value.\n",
    "        warmup_proportion: 0 < warmup_proportion < 1. The proportion of increasing steps.\n",
    "        min_lr: float >= 0. Minimum learning rate after warmup.\n",
    "    # References\n",
    "        - [Adam - A Method for Stochastic Optimization](https://arxiv.org/abs/1412.6980v8)\n",
    "        - [On the Convergence of Adam and Beyond](https://openreview.net/forum?id=ryQu7f-RZ)\n",
    "        - [On The Variance Of The Adaptive Learning Rate And Beyond](https://arxiv.org/pdf/1908.03265v1.pdf)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, learning_rate=0.001, beta_1=0.9, beta_2=0.999,\n",
    "                 epsilon=None, decay=0., weight_decay=0., amsgrad=False,\n",
    "                 total_steps=0, warmup_proportion=0.1, min_lr=0., **kwargs):\n",
    "        learning_rate = kwargs.pop('lr', learning_rate)\n",
    "        super(RAdam, self).__init__(**kwargs)\n",
    "        with K.name_scope(self.__class__.__name__):\n",
    "            self.iterations = K.variable(0, dtype='int64', name='iterations')\n",
    "            self.learning_rate = K.variable(learning_rate, name='learning_rate')\n",
    "            self.beta_1 = K.variable(beta_1, name='beta_1')\n",
    "            self.beta_2 = K.variable(beta_2, name='beta_2')\n",
    "            self.decay = K.variable(decay, name='decay')\n",
    "            self.weight_decay = K.variable(weight_decay, name='weight_decay')\n",
    "            self.total_steps = K.variable(total_steps, name='total_steps')\n",
    "            self.warmup_proportion = K.variable(warmup_proportion, name='warmup_proportion')\n",
    "            self.min_lr = K.variable(min_lr, name='min_lr')\n",
    "        if epsilon is None:\n",
    "            epsilon = K.epsilon()\n",
    "        self.epsilon = epsilon\n",
    "        self.initial_decay = decay\n",
    "        self.initial_weight_decay = weight_decay\n",
    "        self.initial_total_steps = total_steps\n",
    "        self.amsgrad = amsgrad\n",
    "\n",
    "    def get_updates(self, loss, params):\n",
    "        grads = self.get_gradients(loss, params)\n",
    "        self.updates = [K.update_add(self.iterations, 1)]\n",
    "\n",
    "        lr = self.lr\n",
    "\n",
    "        if self.initial_decay > 0:\n",
    "            lr = lr * (1. / (1. + self.decay * K.cast(self.iterations, K.dtype(self.decay))))\n",
    "\n",
    "        t = K.cast(self.iterations, K.floatx()) + 1\n",
    "\n",
    "        if self.initial_total_steps > 0:\n",
    "            warmup_steps = self.total_steps * self.warmup_proportion\n",
    "            decay_steps = K.maximum(self.total_steps - warmup_steps, 1)\n",
    "            decay_rate = (self.min_lr - lr) / decay_steps\n",
    "            lr = K.switch(\n",
    "                t <= warmup_steps,\n",
    "                lr * (t / warmup_steps),\n",
    "                lr + decay_rate * K.minimum(t - warmup_steps, decay_steps),\n",
    "            )\n",
    "\n",
    "        ms = [K.zeros(K.int_shape(p), dtype=K.dtype(p), name='m_' + str(i)) for (i, p) in enumerate(params)]\n",
    "        vs = [K.zeros(K.int_shape(p), dtype=K.dtype(p), name='v_' + str(i)) for (i, p) in enumerate(params)]\n",
    "\n",
    "        if self.amsgrad:\n",
    "            vhats = [K.zeros(K.int_shape(p), dtype=K.dtype(p), name='vhat_' + str(i)) for (i, p) in enumerate(params)]\n",
    "        else:\n",
    "            vhats = [K.zeros(1, name='vhat_' + str(i)) for i in range(len(params))]\n",
    "\n",
    "        self.weights = [self.iterations] + ms + vs + vhats\n",
    "\n",
    "        beta_1_t = K.pow(self.beta_1, t)\n",
    "        beta_2_t = K.pow(self.beta_2, t)\n",
    "\n",
    "        sma_inf = 2.0 / (1.0 - self.beta_2) - 1.0\n",
    "        sma_t = sma_inf - 2.0 * t * beta_2_t / (1.0 - beta_2_t)\n",
    "\n",
    "        for p, g, m, v, vhat in zip(params, grads, ms, vs, vhats):\n",
    "            m_t = (self.beta_1 * m) + (1. - self.beta_1) * g\n",
    "            v_t = (self.beta_2 * v) + (1. - self.beta_2) * K.square(g)\n",
    "\n",
    "            m_corr_t = m_t / (1.0 - beta_1_t)\n",
    "            if self.amsgrad:\n",
    "                vhat_t = K.maximum(vhat, v_t)\n",
    "                v_corr_t = K.sqrt(vhat_t / (1.0 - beta_2_t))\n",
    "                self.updates.append(K.update(vhat, vhat_t))\n",
    "            else:\n",
    "                v_corr_t = K.sqrt(v_t / (1.0 - beta_2_t))\n",
    "\n",
    "            r_t = K.sqrt((sma_t - 4.0) / (sma_inf - 4.0) *\n",
    "                         (sma_t - 2.0) / (sma_inf - 2.0) *\n",
    "                         sma_inf / sma_t)\n",
    "\n",
    "            p_t = K.switch(sma_t >= 5, r_t * m_corr_t / (v_corr_t + self.epsilon), m_corr_t)\n",
    "\n",
    "            if self.initial_weight_decay > 0:\n",
    "                p_t += self.weight_decay * p\n",
    "\n",
    "            p_t = p - lr * p_t\n",
    "\n",
    "            self.updates.append(K.update(m, m_t))\n",
    "            self.updates.append(K.update(v, v_t))\n",
    "            new_p = p_t\n",
    "\n",
    "            # Apply constraints.\n",
    "            if getattr(p, 'constraint', None) is not None:\n",
    "                new_p = p.constraint(new_p)\n",
    "\n",
    "            self.updates.append(K.update(p, new_p))\n",
    "        return self.updates\n",
    "\n",
    "    @property\n",
    "    def lr(self):\n",
    "        return self.learning_rate\n",
    "\n",
    "    @lr.setter\n",
    "    def lr(self, learning_rate):\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            'learning_rate': float(K.get_value(self.learning_rate)),\n",
    "            'beta_1': float(K.get_value(self.beta_1)),\n",
    "            'beta_2': float(K.get_value(self.beta_2)),\n",
    "            'decay': float(K.get_value(self.decay)),\n",
    "            'weight_decay': float(K.get_value(self.weight_decay)),\n",
    "            'epsilon': self.epsilon,\n",
    "            'amsgrad': self.amsgrad,\n",
    "            'total_steps': float(K.get_value(self.total_steps)),\n",
    "            'warmup_proportion': float(K.get_value(self.warmup_proportion)),\n",
    "            'min_lr': float(K.get_value(self.min_lr)),\n",
    "        }\n",
    "        base_config = super(RAdam, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from https://www.kaggle.com/davidcairuz/nfl-neural-network-w-softmax\n",
    "def crps(y_true, y_pred):\n",
    "    return K.mean(K.square(y_true - K.cumsum(y_pred, axis=1)), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    x = keras.layers.Input(shape=[X_train.shape[1]])\n",
    "    fc1 = keras.layers.Dense(units=450, input_shape=[X_train.shape[1]])(x)\n",
    "    act1 = keras.layers.PReLU()(fc1)\n",
    "    bn1 = keras.layers.BatchNormalization()(act1)\n",
    "    dp1 = keras.layers.Dropout(0.55)(bn1)\n",
    "    concat1 = keras.layers.Concatenate()([x, dp1])\n",
    "    fc2 = keras.layers.Dense(units=600)(concat1)\n",
    "    act2 = keras.layers.PReLU()(fc2)\n",
    "    bn2 = keras.layers.BatchNormalization()(act2)\n",
    "    dp2 = keras.layers.Dropout(0.55)(bn2)\n",
    "    concat2 = keras.layers.Concatenate()([concat1, dp2])\n",
    "    fc3 = keras.layers.Dense(units=400)(concat2)\n",
    "    act3 = keras.layers.PReLU()(fc3)\n",
    "    bn3 = keras.layers.BatchNormalization()(act3)\n",
    "    dp3 = keras.layers.Dropout(0.55)(bn3)\n",
    "    concat3 = keras.layers.Concatenate([concat2, dp3])\n",
    "    output = keras.layers.Dense(units=199, activation='softmax')(concat2)\n",
    "    model = keras.models.Model(inputs=[x], outputs=[output])\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_model(X_train, y_train, X_val, y_val):\n",
    "    model = get_model()\n",
    "    model.compile(optimizer=RAdam(warmup_proportion=0.1, min_lr=1e-7), loss=crps)\n",
    "    er = EarlyStopping(patience=20, min_delta=1e-4, restore_best_weights=True, monitor='val_loss')\n",
    "    model.fit(X_train, y_train, epochs=200, callbacks=[er], validation_data=[X_val, y_val], batch_size=batch_size)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RepeatedKFold\n",
    "\n",
    "rkf = RepeatedKFold(n_splits=4, n_repeats=6)\n",
    "\n",
    "models = []\n",
    "\n",
    "for tr_idx, vl_idx in rkf.split(X_train, y_train):\n",
    "    \n",
    "    x_tr, y_tr = X_train[tr_idx], y_train[tr_idx]\n",
    "    x_vl, y_vl = X_train[vl_idx], y_train[vl_idx]\n",
    "    \n",
    "    model = train_model(x_tr, y_tr, x_vl, y_vl)\n",
    "    models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pred(df, sample, env, models):\n",
    "    df['StadiumType'] = df['StadiumType'].apply(clean_StadiumType)\n",
    "    df['StadiumType'] = df['StadiumType'].apply(transform_StadiumType)\n",
    "    df['DefendersInTheBox_vs_Distance'] = df['DefendersInTheBox'] / df['Distance']\n",
    "    df['OffenseFormation'] = df['OffenseFormation'].apply(lambda x: x if x in off_form else np.nan)\n",
    "    df = pd.concat([df.drop(['OffenseFormation'], axis=1), pd.get_dummies(df['OffenseFormation'], prefix='Formation')], axis=1)\n",
    "    missing_cols = set( dummy_col ) - set( df.columns )-set('Yards')\n",
    "    for c in missing_cols:\n",
    "        df[c] = 0\n",
    "    df = df[dummy_col]\n",
    "    df.drop(['Yards'], axis=1, inplace=True)\n",
    "    df['Turf'] = df['Turf'].map(Turf)\n",
    "    df['Turf'] = df['Turf'] == 'Natural'\n",
    "    df['PossessionTeam'] = df['PossessionTeam'].map(map_abbr)\n",
    "    df['HomeTeamAbbr'] = df['HomeTeamAbbr'].map(map_abbr)\n",
    "    df['VisitorTeamAbbr'] = df['VisitorTeamAbbr'].map(map_abbr)\n",
    "    df['HomePossesion'] = df['PossessionTeam'] == df['HomeTeamAbbr']\n",
    "    df['Field_eq_Possession'] = df['FieldPosition'] == df['PossessionTeam']\n",
    "    df['HomeField'] = df['FieldPosition'] == df['HomeTeamAbbr']\n",
    "    df['GameClock'] = df['GameClock'].apply(strtoseconds)\n",
    "    df['PlayerHeight'] = df['PlayerHeight'].apply(lambda x: 12*int(x.split('-')[0])+int(x.split('-')[1]))\n",
    "    df['PlayerBMI'] = 703*(df['PlayerWeight']/(df['PlayerHeight'])**2)\n",
    "    df['TimeHandoff'] = df['TimeHandoff'].apply(lambda x: datetime.datetime.strptime(x, \"%Y-%m-%dT%H:%M:%S.%fZ\"))\n",
    "    df['TimeSnap'] = df['TimeSnap'].apply(lambda x: datetime.datetime.strptime(x, \"%Y-%m-%dT%H:%M:%S.%fZ\"))\n",
    "    df['TimeDelta'] = df.apply(lambda row: (row['TimeHandoff'] - row['TimeSnap']).total_seconds(), axis=1)\n",
    "    df['PlayerBirthDate'] = df['PlayerBirthDate'].apply(lambda x: datetime.datetime.strptime(x, \"%m/%d/%Y\"))\n",
    "    seconds_in_year = 60*60*24*365.25\n",
    "    df['PlayerAge'] = df.apply(lambda row: (row['TimeHandoff']-row['PlayerBirthDate']).total_seconds()/seconds_in_year, axis=1)\n",
    "    df['WindSpeed'] = df['WindSpeed'].apply(lambda x: x.lower().replace('mph', '').strip() if not pd.isna(x) else x)\n",
    "    df['WindSpeed'] = df['WindSpeed'].apply(lambda x: (int(x.split('-')[0])+int(x.split('-')[1]))/2 if not pd.isna(x) and '-' in x else x)\n",
    "    df['WindSpeed'] = df['WindSpeed'].apply(lambda x: (int(x.split()[0])+int(x.split()[-1]))/2 if not pd.isna(x) and type(x)!=float and 'gusts up to' in x else x)\n",
    "    df['WindSpeed'] = df['WindSpeed'].apply(str_to_float)\n",
    "    df['WindDirection'] = df['WindDirection'].apply(clean_WindDirection)\n",
    "    df['WindDirection'] = df['WindDirection'].apply(transform_WindDirection)\n",
    "    df['PlayDirection'] = df['PlayDirection'].apply(lambda x: x.strip() == 'right')\n",
    "    df['Team'] = df['Team'].apply(lambda x: x.strip()=='home')\n",
    "    indoor = \"indoor\"\n",
    "    df['GameWeather'] = df['GameWeather'].apply(lambda x: indoor if not pd.isna(x) and indoor in x else x)\n",
    "    df['GameWeather'] = df['GameWeather'].apply(lambda x: x.lower().replace('coudy', 'cloudy').replace('clouidy', 'cloudy').replace('party', 'partly').replace('clear and sunny', 'sunny and clear').replace('skies', '').replace(\"mostly\", \"\").strip() if not pd.isna(x) else x)\n",
    "    df['GameWeather'] = df['GameWeather'].apply(map_weather)\n",
    "    df['IsRusher'] = df['NflId'] == df['NflIdRusher']\n",
    "    df['X'] = df.apply(lambda row: row['X'] if row['PlayDirection'] else 120-row['X'], axis=1)\n",
    "    df['Orientation'] = df.apply(lambda row: new_orientation(row['Orientation'], row['PlayDirection']), axis=1)\n",
    "    df['Dir'] = df.apply(lambda row: new_orientation(row['Dir'], row['PlayDirection']), axis=1)\n",
    "    df['YardsLeft'] = df.apply(lambda row: 100-row['YardLine'] if row['HomeField'] else row['YardLine'], axis=1)\n",
    "    df['YardsLeft'] = df.apply(lambda row: row['YardsLeft'] if row['PlayDirection'] else 100-row['YardsLeft'], axis=1)\n",
    "    df = df.sort_values(by=['PlayId', 'Team', 'IsRusher', 'JerseyNumber']).reset_index()\n",
    "    df = df.drop(['TimeHandoff', 'TimeSnap', 'PlayerBirthDate', 'NflId', 'NflIdRusher', 'GameId', 'PlayId', 'index', 'IsRusher', 'Team'], axis=1)\n",
    "    cat_features = []\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype =='object':\n",
    "            cat_features.append(col)\n",
    "\n",
    "    df = df.drop(cat_features, axis=1)\n",
    "    df.fillna(-999, inplace=True)\n",
    "    X = np.array(df[players_col]).reshape(-1, len(players_col)*22)\n",
    "    play_col = df.drop(players_col, axis=1).columns\n",
    "    X_play_col = np.zeros(shape=(X.shape[0], len(play_col)))\n",
    "    for i, col in enumerate(play_col):\n",
    "        X_play_col[:, i] = df[col][::22]\n",
    "    X = np.concatenate([X, X_play_col], axis=1)\n",
    "    X = scaler.transform(X)\n",
    "    y_pred = np.mean([np.cumsum(model.predict(X), axis=1) for model in models], axis=0)\n",
    "    yardsleft = np.array(df['YardsLeft'][::22])\n",
    "    \n",
    "    for i in range(len(yardsleft)):\n",
    "        y_pred[i, :yardsleft[i]-1] = 0\n",
    "        y_pred[i, yardsleft[i]+100:] = 1\n",
    "    env.predict(pd.DataFrame(data=y_pred.clip(0,1),columns=sample.columns))\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for test, sample in tqdm.tqdm(env.iter_test()):\n",
    "     make_pred(test, sample, env, models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.write_submission_file()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b5211633-aa3d-4655-bb8b-ab24801ffef7",
    "_uuid": "6029f120-f8c5-4933-b02d-774383cd2164"
   },
   "source": [
    "# End\n",
    "\n",
    "If you reached this far please comment and upvote this kernel, feel free to make improvements on the kernel and please share if you found anything useful!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
