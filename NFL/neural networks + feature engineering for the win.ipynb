{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "7733b156-760c-464c-bd84-2fa4b5af23ad",
    "_uuid": "b2c90bb4-a341-4c20-b0aa-4f0c05d57b4a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARI ARZ\n",
      "BAL BLT\n",
      "CLE CLV\n",
      "HOU HST\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1029 14:50:24.565797  2248 nn_ops.py:4220] Large dropout rate: 0.55 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "W1029 14:50:24.691742  2248 nn_ops.py:4220] Large dropout rate: 0.55 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "W1029 14:50:24.803717  2248 nn_ops.py:4220] Large dropout rate: 0.55 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 17209 samples, validate on 5737 samples\n",
      "Epoch 1/200\n",
      "17209/17209 [==============================] - ETA: 6:52 - loss: 0.088 - ETA: 1:45 - loss: 0.094 - ETA: 53s - loss: 0.095 - ETA: 36s - loss: 0.09 - ETA: 27s - loss: 0.09 - ETA: 22s - loss: 0.09 - ETA: 19s - loss: 0.09 - ETA: 16s - loss: 0.09 - ETA: 14s - loss: 0.09 - ETA: 13s - loss: 0.09 - ETA: 12s - loss: 0.09 - ETA: 11s - loss: 0.09 - ETA: 10s - loss: 0.09 - ETA: 9s - loss: 0.0944 - ETA: 8s - loss: 0.094 - ETA: 8s - loss: 0.094 - ETA: 7s - loss: 0.094 - ETA: 7s - loss: 0.094 - ETA: 7s - loss: 0.094 - ETA: 6s - loss: 0.094 - ETA: 6s - loss: 0.094 - ETA: 5s - loss: 0.094 - ETA: 5s - loss: 0.094 - ETA: 5s - loss: 0.094 - ETA: 5s - loss: 0.094 - ETA: 4s - loss: 0.094 - ETA: 4s - loss: 0.094 - ETA: 4s - loss: 0.094 - ETA: 4s - loss: 0.094 - ETA: 4s - loss: 0.094 - ETA: 4s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 1s - loss: 0.095 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.095 - ETA: 1s - loss: 0.095 - ETA: 1s - loss: 0.095 - ETA: 1s - loss: 0.095 - ETA: 1s - loss: 0.095 - ETA: 1s - loss: 0.095 - ETA: 1s - loss: 0.095 - ETA: 1s - loss: 0.095 - ETA: 0s - loss: 0.095 - ETA: 0s - loss: 0.095 - ETA: 0s - loss: 0.095 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - 6s 376us/step - loss: 0.0949 - val_loss: 0.0879\n",
      "Epoch 2/200\n",
      "17209/17209 [==============================] - ETA: 4s - loss: 0.097 - ETA: 4s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.095 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.095 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.095 - ETA: 3s - loss: 0.095 - ETA: 3s - loss: 0.095 - ETA: 3s - loss: 0.095 - ETA: 3s - loss: 0.095 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.095 - ETA: 2s - loss: 0.095 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.095 - ETA: 2s - loss: 0.095 - ETA: 2s - loss: 0.095 - ETA: 2s - loss: 0.095 - ETA: 2s - loss: 0.095 - ETA: 2s - loss: 0.095 - ETA: 2s - loss: 0.095 - ETA: 2s - loss: 0.095 - ETA: 2s - loss: 0.095 - ETA: 2s - loss: 0.095 - ETA: 2s - loss: 0.095 - ETA: 2s - loss: 0.095 - ETA: 2s - loss: 0.095 - ETA: 1s - loss: 0.095 - ETA: 1s - loss: 0.095 - ETA: 1s - loss: 0.095 - ETA: 1s - loss: 0.095 - ETA: 1s - loss: 0.095 - ETA: 1s - loss: 0.095 - ETA: 1s - loss: 0.095 - ETA: 1s - loss: 0.095 - ETA: 1s - loss: 0.095 - ETA: 1s - loss: 0.095 - ETA: 1s - loss: 0.095 - ETA: 1s - loss: 0.095 - ETA: 1s - loss: 0.095 - ETA: 1s - loss: 0.095 - ETA: 1s - loss: 0.095 - ETA: 1s - loss: 0.095 - ETA: 1s - loss: 0.095 - ETA: 1s - loss: 0.095 - ETA: 1s - loss: 0.095 - ETA: 1s - loss: 0.095 - ETA: 1s - loss: 0.095 - ETA: 1s - loss: 0.095 - ETA: 1s - loss: 0.095 - ETA: 1s - loss: 0.095 - ETA: 0s - loss: 0.095 - ETA: 0s - loss: 0.095 - ETA: 0s - loss: 0.095 - ETA: 0s - loss: 0.095 - ETA: 0s - loss: 0.095 - ETA: 0s - loss: 0.095 - ETA: 0s - loss: 0.095 - ETA: 0s - loss: 0.095 - ETA: 0s - loss: 0.095 - ETA: 0s - loss: 0.095 - ETA: 0s - loss: 0.095 - ETA: 0s - loss: 0.095 - ETA: 0s - loss: 0.095 - ETA: 0s - loss: 0.095 - ETA: 0s - loss: 0.095 - ETA: 0s - loss: 0.095 - ETA: 0s - loss: 0.095 - ETA: 0s - loss: 0.095 - ETA: 0s - loss: 0.095 - ETA: 0s - loss: 0.095 - 5s 268us/step - loss: 0.0950 - val_loss: 0.0882\n",
      "Epoch 3/200\n",
      "17209/17209 [==============================] - ETA: 4s - loss: 0.090 - ETA: 4s - loss: 0.094 - ETA: 4s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.095 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.095 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.095 - ETA: 3s - loss: 0.095 - ETA: 3s - loss: 0.095 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.095 - ETA: 2s - loss: 0.095 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.095 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - 5s 265us/step - loss: 0.0948 - val_loss: 0.0883\n",
      "Epoch 4/200\n",
      "17209/17209 [==============================] - ETA: 4s - loss: 0.093 - ETA: 4s - loss: 0.094 - ETA: 5s - loss: 0.094 - ETA: 5s - loss: 0.094 - ETA: 5s - loss: 0.094 - ETA: 5s - loss: 0.094 - ETA: 5s - loss: 0.094 - ETA: 5s - loss: 0.094 - ETA: 5s - loss: 0.094 - ETA: 4s - loss: 0.094 - ETA: 4s - loss: 0.094 - ETA: 4s - loss: 0.094 - ETA: 4s - loss: 0.094 - ETA: 4s - loss: 0.094 - ETA: 4s - loss: 0.094 - ETA: 4s - loss: 0.094 - ETA: 4s - loss: 0.094 - ETA: 4s - loss: 0.094 - ETA: 4s - loss: 0.094 - ETA: 4s - loss: 0.094 - ETA: 4s - loss: 0.094 - ETA: 4s - loss: 0.094 - ETA: 4s - loss: 0.094 - ETA: 4s - loss: 0.094 - ETA: 4s - loss: 0.094 - ETA: 4s - loss: 0.094 - ETA: 4s - loss: 0.094 - ETA: 4s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - 5s 280us/step - loss: 0.0947 - val_loss: 0.0882\n",
      "Epoch 5/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17209/17209 [==============================] - ETA: 4s - loss: 0.097 - ETA: 4s - loss: 0.097 - ETA: 3s - loss: 0.097 - ETA: 3s - loss: 0.096 - ETA: 3s - loss: 0.095 - ETA: 3s - loss: 0.095 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.095 - ETA: 3s - loss: 0.095 - ETA: 3s - loss: 0.095 - ETA: 3s - loss: 0.095 - ETA: 3s - loss: 0.095 - ETA: 3s - loss: 0.095 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.095 - ETA: 2s - loss: 0.095 - ETA: 2s - loss: 0.095 - ETA: 2s - loss: 0.095 - ETA: 2s - loss: 0.095 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - 4s 239us/step - loss: 0.0947 - val_loss: 0.0882\n",
      "Epoch 6/200\n",
      "17209/17209 [==============================] - ETA: 4s - loss: 0.098 - ETA: 4s - loss: 0.093 - ETA: 4s - loss: 0.094 - ETA: 4s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.093 - ETA: 3s - loss: 0.093 - ETA: 3s - loss: 0.093 - ETA: 3s - loss: 0.093 - ETA: 3s - loss: 0.093 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.095 - ETA: 1s - loss: 0.095 - ETA: 1s - loss: 0.095 - ETA: 1s - loss: 0.095 - ETA: 1s - loss: 0.095 - ETA: 1s - loss: 0.095 - ETA: 0s - loss: 0.095 - ETA: 0s - loss: 0.095 - ETA: 0s - loss: 0.095 - ETA: 0s - loss: 0.095 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.095 - ETA: 0s - loss: 0.095 - ETA: 0s - loss: 0.095 - ETA: 0s - loss: 0.095 - ETA: 0s - loss: 0.095 - ETA: 0s - loss: 0.095 - ETA: 0s - loss: 0.095 - ETA: 0s - loss: 0.095 - ETA: 0s - loss: 0.095 - ETA: 0s - loss: 0.095 - ETA: 0s - loss: 0.095 - ETA: 0s - loss: 0.095 - 4s 242us/step - loss: 0.0950 - val_loss: 0.0882\n",
      "Epoch 7/200\n",
      "17209/17209 [==============================] - ETA: 4s - loss: 0.093 - ETA: 4s - loss: 0.093 - ETA: 3s - loss: 0.093 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - 4s 241us/step - loss: 0.0947 - val_loss: 0.0882\n",
      "Epoch 8/200\n",
      "17209/17209 [==============================] - ETA: 4s - loss: 0.094 - ETA: 4s - loss: 0.092 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.093 - ETA: 3s - loss: 0.093 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - 4s 240us/step - loss: 0.0947 - val_loss: 0.0882\n",
      "Epoch 9/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17209/17209 [==============================] - ETA: 4s - loss: 0.089 - ETA: 4s - loss: 0.096 - ETA: 3s - loss: 0.097 - ETA: 3s - loss: 0.096 - ETA: 3s - loss: 0.095 - ETA: 3s - loss: 0.095 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.095 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.095 - ETA: 2s - loss: 0.095 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.095 - ETA: 2s - loss: 0.095 - ETA: 2s - loss: 0.095 - ETA: 2s - loss: 0.095 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.095 - ETA: 2s - loss: 0.095 - ETA: 2s - loss: 0.095 - ETA: 2s - loss: 0.095 - ETA: 1s - loss: 0.095 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - 4s 235us/step - loss: 0.0946 - val_loss: 0.0882\n",
      "Epoch 10/200\n",
      "17209/17209 [==============================] - ETA: 4s - loss: 0.095 - ETA: 3s - loss: 0.095 - ETA: 3s - loss: 0.095 - ETA: 3s - loss: 0.095 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.095 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.095 - ETA: 3s - loss: 0.095 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.095 - ETA: 0s - loss: 0.095 - ETA: 0s - loss: 0.095 - ETA: 0s - loss: 0.095 - ETA: 0s - loss: 0.095 - ETA: 0s - loss: 0.095 - 4s 241us/step - loss: 0.0951 - val_loss: 0.0882\n",
      "Epoch 11/200\n",
      "17209/17209 [==============================] - ETA: 3s - loss: 0.096 - ETA: 3s - loss: 0.095 - ETA: 3s - loss: 0.093 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.095 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.095 - ETA: 3s - loss: 0.095 - ETA: 3s - loss: 0.095 - ETA: 3s - loss: 0.095 - ETA: 3s - loss: 0.095 - ETA: 3s - loss: 0.095 - ETA: 3s - loss: 0.095 - ETA: 3s - loss: 0.095 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.095 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - 4s 235us/step - loss: 0.0947 - val_loss: 0.0882\n",
      "Epoch 12/200\n",
      "17209/17209 [==============================] - ETA: 4s - loss: 0.091 - ETA: 3s - loss: 0.093 - ETA: 3s - loss: 0.093 - ETA: 3s - loss: 0.093 - ETA: 3s - loss: 0.093 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - 4s 240us/step - loss: 0.0947 - val_loss: 0.0882\n",
      "Epoch 13/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17209/17209 [==============================] - ETA: 4s - loss: 0.093 - ETA: 4s - loss: 0.095 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.095 - ETA: 3s - loss: 0.095 - ETA: 3s - loss: 0.095 - ETA: 3s - loss: 0.095 - ETA: 3s - loss: 0.095 - ETA: 3s - loss: 0.095 - ETA: 3s - loss: 0.095 - ETA: 3s - loss: 0.095 - ETA: 3s - loss: 0.095 - ETA: 3s - loss: 0.095 - ETA: 3s - loss: 0.095 - ETA: 3s - loss: 0.095 - ETA: 3s - loss: 0.095 - ETA: 2s - loss: 0.095 - ETA: 2s - loss: 0.095 - ETA: 2s - loss: 0.095 - ETA: 2s - loss: 0.095 - ETA: 2s - loss: 0.095 - ETA: 2s - loss: 0.095 - ETA: 2s - loss: 0.095 - ETA: 2s - loss: 0.095 - ETA: 2s - loss: 0.095 - ETA: 2s - loss: 0.095 - ETA: 2s - loss: 0.095 - ETA: 2s - loss: 0.095 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.095 - ETA: 2s - loss: 0.095 - ETA: 2s - loss: 0.095 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 1s - loss: 0.095 - ETA: 1s - loss: 0.095 - ETA: 1s - loss: 0.095 - ETA: 1s - loss: 0.095 - ETA: 1s - loss: 0.095 - ETA: 1s - loss: 0.095 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - 4s 242us/step - loss: 0.0948 - val_loss: 0.0882\n",
      "Epoch 14/200\n",
      "17209/17209 [==============================] - ETA: 4s - loss: 0.103 - ETA: 4s - loss: 0.095 - ETA: 4s - loss: 0.095 - ETA: 4s - loss: 0.095 - ETA: 3s - loss: 0.095 - ETA: 3s - loss: 0.095 - ETA: 3s - loss: 0.095 - ETA: 3s - loss: 0.095 - ETA: 3s - loss: 0.095 - ETA: 3s - loss: 0.095 - ETA: 3s - loss: 0.095 - ETA: 3s - loss: 0.095 - ETA: 3s - loss: 0.095 - ETA: 3s - loss: 0.095 - ETA: 3s - loss: 0.095 - ETA: 3s - loss: 0.095 - ETA: 3s - loss: 0.095 - ETA: 3s - loss: 0.095 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.095 - ETA: 3s - loss: 0.095 - ETA: 3s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - 4s 241us/step - loss: 0.0947 - val_loss: 0.0883\n",
      "Epoch 15/200\n",
      "17209/17209 [==============================] - ETA: 4s - loss: 0.092 - ETA: 4s - loss: 0.094 - ETA: 3s - loss: 0.093 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.095 - ETA: 3s - loss: 0.095 - ETA: 2s - loss: 0.095 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - 5s 271us/step - loss: 0.0945 - val_loss: 0.0882\n",
      "Epoch 16/200\n",
      "17209/17209 [==============================] - ETA: 5s - loss: 0.092 - ETA: 5s - loss: 0.096 - ETA: 4s - loss: 0.095 - ETA: 4s - loss: 0.095 - ETA: 4s - loss: 0.095 - ETA: 4s - loss: 0.095 - ETA: 4s - loss: 0.095 - ETA: 4s - loss: 0.096 - ETA: 4s - loss: 0.096 - ETA: 4s - loss: 0.095 - ETA: 4s - loss: 0.095 - ETA: 4s - loss: 0.095 - ETA: 4s - loss: 0.095 - ETA: 4s - loss: 0.095 - ETA: 3s - loss: 0.095 - ETA: 3s - loss: 0.095 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.095 - ETA: 1s - loss: 0.095 - ETA: 1s - loss: 0.095 - ETA: 1s - loss: 0.095 - ETA: 1s - loss: 0.095 - ETA: 1s - loss: 0.095 - ETA: 1s - loss: 0.095 - ETA: 0s - loss: 0.095 - ETA: 0s - loss: 0.095 - ETA: 0s - loss: 0.095 - ETA: 0s - loss: 0.095 - ETA: 0s - loss: 0.095 - ETA: 0s - loss: 0.095 - ETA: 0s - loss: 0.095 - ETA: 0s - loss: 0.095 - ETA: 0s - loss: 0.095 - ETA: 0s - loss: 0.095 - ETA: 0s - loss: 0.095 - ETA: 0s - loss: 0.095 - ETA: 0s - loss: 0.095 - ETA: 0s - loss: 0.095 - ETA: 0s - loss: 0.095 - 5s 273us/step - loss: 0.0951 - val_loss: 0.0882\n",
      "Epoch 17/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17209/17209 [==============================] - ETA: 4s - loss: 0.097 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.095 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.095 - ETA: 2s - loss: 0.095 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.095 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - 4s 248us/step - loss: 0.0948 - val_loss: 0.0883\n",
      "Epoch 18/200\n",
      "17209/17209 [==============================] - ETA: 3s - loss: 0.093 - ETA: 3s - loss: 0.095 - ETA: 3s - loss: 0.095 - ETA: 3s - loss: 0.096 - ETA: 3s - loss: 0.096 - ETA: 3s - loss: 0.096 - ETA: 3s - loss: 0.096 - ETA: 3s - loss: 0.096 - ETA: 3s - loss: 0.096 - ETA: 3s - loss: 0.096 - ETA: 3s - loss: 0.095 - ETA: 3s - loss: 0.095 - ETA: 3s - loss: 0.095 - ETA: 3s - loss: 0.095 - ETA: 3s - loss: 0.095 - ETA: 3s - loss: 0.095 - ETA: 3s - loss: 0.095 - ETA: 3s - loss: 0.095 - ETA: 3s - loss: 0.095 - ETA: 2s - loss: 0.095 - ETA: 2s - loss: 0.095 - ETA: 2s - loss: 0.095 - ETA: 2s - loss: 0.095 - ETA: 2s - loss: 0.095 - ETA: 2s - loss: 0.095 - ETA: 2s - loss: 0.095 - ETA: 2s - loss: 0.095 - ETA: 2s - loss: 0.095 - ETA: 2s - loss: 0.095 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.095 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.095 - ETA: 1s - loss: 0.095 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.095 - ETA: 1s - loss: 0.095 - ETA: 1s - loss: 0.095 - ETA: 1s - loss: 0.095 - ETA: 1s - loss: 0.095 - ETA: 1s - loss: 0.095 - ETA: 1s - loss: 0.095 - ETA: 0s - loss: 0.095 - ETA: 0s - loss: 0.095 - ETA: 0s - loss: 0.095 - ETA: 0s - loss: 0.095 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.095 - ETA: 0s - loss: 0.095 - ETA: 0s - loss: 0.095 - ETA: 0s - loss: 0.095 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - 5s 269us/step - loss: 0.0948 - val_loss: 0.0882\n",
      "Epoch 19/200\n",
      "17209/17209 [==============================] - ETA: 3s - loss: 0.095 - ETA: 3s - loss: 0.093 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.095 - ETA: 3s - loss: 0.095 - ETA: 3s - loss: 0.095 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - 4s 235us/step - loss: 0.0946 - val_loss: 0.0882\n",
      "Epoch 20/200\n",
      "17209/17209 [==============================] - ETA: 4s - loss: 0.097 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 1s - loss: 0.095 - ETA: 1s - loss: 0.095 - ETA: 1s - loss: 0.095 - ETA: 1s - loss: 0.095 - ETA: 1s - loss: 0.095 - ETA: 1s - loss: 0.095 - ETA: 1s - loss: 0.095 - ETA: 1s - loss: 0.095 - ETA: 1s - loss: 0.095 - ETA: 1s - loss: 0.095 - ETA: 1s - loss: 0.095 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.095 - ETA: 1s - loss: 0.095 - ETA: 1s - loss: 0.095 - ETA: 1s - loss: 0.095 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 0s - loss: 0.095 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.095 - ETA: 0s - loss: 0.095 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - 4s 233us/step - loss: 0.0948 - val_loss: 0.0882\n",
      "Epoch 21/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17209/17209 [==============================] - ETA: 4s - loss: 0.091 - ETA: 3s - loss: 0.092 - ETA: 3s - loss: 0.092 - ETA: 3s - loss: 0.092 - ETA: 3s - loss: 0.092 - ETA: 3s - loss: 0.093 - ETA: 3s - loss: 0.093 - ETA: 3s - loss: 0.093 - ETA: 3s - loss: 0.093 - ETA: 3s - loss: 0.093 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - 4s 228us/step - loss: 0.0946 - val_loss: 0.0882\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1029 14:51:59.010466  2248 nn_ops.py:4220] Large dropout rate: 0.55 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "W1029 14:51:59.109490  2248 nn_ops.py:4220] Large dropout rate: 0.55 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 17209 samples, validate on 5737 samples\n",
      "Epoch 1/200\n",
      "17209/17209 [==============================] - ETA: 3:45 - loss: 0.095 - ETA: 47s - loss: 0.094 - ETA: 27s - loss: 0.09 - ETA: 20s - loss: 0.09 - ETA: 16s - loss: 0.09 - ETA: 13s - loss: 0.09 - ETA: 11s - loss: 0.09 - ETA: 10s - loss: 0.09 - ETA: 9s - loss: 0.0941 - ETA: 8s - loss: 0.094 - ETA: 8s - loss: 0.094 - ETA: 8s - loss: 0.094 - ETA: 7s - loss: 0.094 - ETA: 7s - loss: 0.094 - ETA: 7s - loss: 0.094 - ETA: 6s - loss: 0.094 - ETA: 6s - loss: 0.094 - ETA: 6s - loss: 0.094 - ETA: 5s - loss: 0.094 - ETA: 5s - loss: 0.094 - ETA: 5s - loss: 0.094 - ETA: 5s - loss: 0.094 - ETA: 4s - loss: 0.094 - ETA: 4s - loss: 0.094 - ETA: 4s - loss: 0.094 - ETA: 4s - loss: 0.094 - ETA: 4s - loss: 0.094 - ETA: 4s - loss: 0.094 - ETA: 4s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.093 - ETA: 1s - loss: 0.093 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - 6s 355us/step - loss: 0.0942 - val_loss: 0.0864\n",
      "Epoch 2/200\n",
      "17209/17209 [==============================] - ETA: 4s - loss: 0.094 - ETA: 4s - loss: 0.094 - ETA: 4s - loss: 0.093 - ETA: 4s - loss: 0.093 - ETA: 3s - loss: 0.093 - ETA: 3s - loss: 0.093 - ETA: 3s - loss: 0.093 - ETA: 3s - loss: 0.093 - ETA: 3s - loss: 0.093 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.093 - ETA: 3s - loss: 0.093 - ETA: 3s - loss: 0.093 - ETA: 3s - loss: 0.093 - ETA: 3s - loss: 0.093 - ETA: 3s - loss: 0.093 - ETA: 3s - loss: 0.093 - ETA: 3s - loss: 0.093 - ETA: 3s - loss: 0.093 - ETA: 3s - loss: 0.093 - ETA: 3s - loss: 0.093 - ETA: 3s - loss: 0.093 - ETA: 3s - loss: 0.093 - ETA: 2s - loss: 0.093 - ETA: 2s - loss: 0.093 - ETA: 2s - loss: 0.093 - ETA: 2s - loss: 0.093 - ETA: 2s - loss: 0.093 - ETA: 2s - loss: 0.093 - ETA: 2s - loss: 0.093 - ETA: 2s - loss: 0.093 - ETA: 2s - loss: 0.093 - ETA: 2s - loss: 0.093 - ETA: 2s - loss: 0.093 - ETA: 2s - loss: 0.093 - ETA: 2s - loss: 0.093 - ETA: 1s - loss: 0.093 - ETA: 1s - loss: 0.093 - ETA: 1s - loss: 0.093 - ETA: 1s - loss: 0.093 - ETA: 1s - loss: 0.093 - ETA: 1s - loss: 0.093 - ETA: 1s - loss: 0.093 - ETA: 1s - loss: 0.093 - ETA: 1s - loss: 0.093 - ETA: 1s - loss: 0.093 - ETA: 1s - loss: 0.093 - ETA: 1s - loss: 0.093 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - 4s 242us/step - loss: 0.0944 - val_loss: 0.0871\n",
      "Epoch 3/200\n",
      "17209/17209 [==============================] - ETA: 4s - loss: 0.091 - ETA: 4s - loss: 0.095 - ETA: 4s - loss: 0.094 - ETA: 4s - loss: 0.094 - ETA: 4s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.095 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.095 - ETA: 2s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - 4s 241us/step - loss: 0.0947 - val_loss: 0.0871\n",
      "Epoch 4/200\n",
      "17209/17209 [==============================] - ETA: 3s - loss: 0.090 - ETA: 3s - loss: 0.093 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.093 - ETA: 3s - loss: 0.093 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.093 - ETA: 3s - loss: 0.093 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.093 - ETA: 2s - loss: 0.093 - ETA: 2s - loss: 0.093 - ETA: 2s - loss: 0.093 - ETA: 2s - loss: 0.093 - ETA: 2s - loss: 0.093 - ETA: 2s - loss: 0.093 - ETA: 2s - loss: 0.093 - ETA: 2s - loss: 0.093 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.093 - ETA: 2s - loss: 0.093 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 1s - loss: 0.093 - ETA: 1s - loss: 0.093 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - 4s 230us/step - loss: 0.0941 - val_loss: 0.0871\n",
      "Epoch 5/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17209/17209 [==============================] - ETA: 4s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.095 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.095 - ETA: 3s - loss: 0.095 - ETA: 3s - loss: 0.095 - ETA: 3s - loss: 0.096 - ETA: 3s - loss: 0.095 - ETA: 3s - loss: 0.095 - ETA: 3s - loss: 0.095 - ETA: 3s - loss: 0.095 - ETA: 2s - loss: 0.095 - ETA: 2s - loss: 0.095 - ETA: 2s - loss: 0.095 - ETA: 2s - loss: 0.095 - ETA: 2s - loss: 0.095 - ETA: 2s - loss: 0.095 - ETA: 2s - loss: 0.095 - ETA: 2s - loss: 0.095 - ETA: 2s - loss: 0.095 - ETA: 2s - loss: 0.095 - ETA: 2s - loss: 0.095 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.095 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.095 - ETA: 1s - loss: 0.095 - ETA: 1s - loss: 0.095 - ETA: 1s - loss: 0.095 - ETA: 1s - loss: 0.095 - ETA: 1s - loss: 0.095 - ETA: 1s - loss: 0.095 - ETA: 1s - loss: 0.095 - ETA: 1s - loss: 0.095 - ETA: 1s - loss: 0.095 - ETA: 1s - loss: 0.095 - ETA: 1s - loss: 0.095 - ETA: 1s - loss: 0.095 - ETA: 0s - loss: 0.095 - ETA: 0s - loss: 0.095 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - 4s 231us/step - loss: 0.0948 - val_loss: 0.0871\n",
      "Epoch 6/200\n",
      "17209/17209 [==============================] - ETA: 3s - loss: 0.093 - ETA: 3s - loss: 0.095 - ETA: 3s - loss: 0.095 - ETA: 3s - loss: 0.095 - ETA: 3s - loss: 0.095 - ETA: 3s - loss: 0.095 - ETA: 3s - loss: 0.095 - ETA: 3s - loss: 0.095 - ETA: 3s - loss: 0.095 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.095 - ETA: 2s - loss: 0.095 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.095 - ETA: 2s - loss: 0.095 - ETA: 2s - loss: 0.095 - ETA: 2s - loss: 0.095 - ETA: 2s - loss: 0.095 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - 4s 247us/step - loss: 0.0948 - val_loss: 0.0871\n",
      "Epoch 7/200\n",
      "17209/17209 [==============================] - ETA: 4s - loss: 0.093 - ETA: 4s - loss: 0.094 - ETA: 4s - loss: 0.094 - ETA: 4s - loss: 0.094 - ETA: 4s - loss: 0.094 - ETA: 4s - loss: 0.094 - ETA: 4s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 4s - loss: 0.094 - ETA: 4s - loss: 0.094 - ETA: 4s - loss: 0.094 - ETA: 4s - loss: 0.094 - ETA: 4s - loss: 0.094 - ETA: 4s - loss: 0.094 - ETA: 4s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - 4s 251us/step - loss: 0.0946 - val_loss: 0.0870\n",
      "Epoch 8/200\n",
      "17209/17209 [==============================] - ETA: 4s - loss: 0.093 - ETA: 3s - loss: 0.095 - ETA: 3s - loss: 0.093 - ETA: 3s - loss: 0.093 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - 4s 254us/step - loss: 0.0943 - val_loss: 0.0870\n",
      "Epoch 9/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17209/17209 [==============================] - ETA: 5s - loss: 0.095 - ETA: 4s - loss: 0.094 - ETA: 4s - loss: 0.093 - ETA: 4s - loss: 0.093 - ETA: 4s - loss: 0.093 - ETA: 4s - loss: 0.094 - ETA: 4s - loss: 0.094 - ETA: 4s - loss: 0.094 - ETA: 4s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - 5s 262us/step - loss: 0.0945 - val_loss: 0.0870\n",
      "Epoch 10/200\n",
      "17209/17209 [==============================] - ETA: 5s - loss: 0.096 - ETA: 4s - loss: 0.095 - ETA: 4s - loss: 0.095 - ETA: 4s - loss: 0.095 - ETA: 4s - loss: 0.095 - ETA: 4s - loss: 0.095 - ETA: 4s - loss: 0.095 - ETA: 4s - loss: 0.095 - ETA: 4s - loss: 0.095 - ETA: 4s - loss: 0.094 - ETA: 4s - loss: 0.094 - ETA: 4s - loss: 0.094 - ETA: 4s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - 5s 285us/step - loss: 0.0944 - val_loss: 0.0871\n",
      "Epoch 11/200\n",
      "17209/17209 [==============================] - ETA: 4s - loss: 0.097 - ETA: 3s - loss: 0.093 - ETA: 3s - loss: 0.092 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.095 - ETA: 3s - loss: 0.095 - ETA: 3s - loss: 0.095 - ETA: 3s - loss: 0.095 - ETA: 3s - loss: 0.095 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - 4s 247us/step - loss: 0.0944 - val_loss: 0.0871\n",
      "Epoch 12/200\n",
      "17209/17209 [==============================] - ETA: 4s - loss: 0.089 - ETA: 3s - loss: 0.093 - ETA: 3s - loss: 0.093 - ETA: 4s - loss: 0.094 - ETA: 4s - loss: 0.094 - ETA: 4s - loss: 0.094 - ETA: 4s - loss: 0.094 - ETA: 4s - loss: 0.094 - ETA: 4s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - 5s 264us/step - loss: 0.0945 - val_loss: 0.0871\n",
      "Epoch 13/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17209/17209 [==============================] - ETA: 4s - loss: 0.094 - ETA: 4s - loss: 0.096 - ETA: 4s - loss: 0.095 - ETA: 3s - loss: 0.096 - ETA: 3s - loss: 0.095 - ETA: 3s - loss: 0.095 - ETA: 3s - loss: 0.095 - ETA: 3s - loss: 0.095 - ETA: 3s - loss: 0.095 - ETA: 3s - loss: 0.095 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.095 - ETA: 3s - loss: 0.095 - ETA: 3s - loss: 0.095 - ETA: 3s - loss: 0.095 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.095 - ETA: 3s - loss: 0.095 - ETA: 2s - loss: 0.095 - ETA: 2s - loss: 0.095 - ETA: 2s - loss: 0.095 - ETA: 2s - loss: 0.095 - ETA: 2s - loss: 0.095 - ETA: 2s - loss: 0.095 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.095 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.095 - ETA: 2s - loss: 0.095 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.095 - ETA: 1s - loss: 0.095 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - 4s 249us/step - loss: 0.0945 - val_loss: 0.0871\n",
      "Epoch 14/200\n",
      "17209/17209 [==============================] - ETA: 4s - loss: 0.092 - ETA: 3s - loss: 0.093 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.095 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - 5s 276us/step - loss: 0.0941 - val_loss: 0.0870\n",
      "Epoch 15/200\n",
      "17209/17209 [==============================] - ETA: 4s - loss: 0.096 - ETA: 4s - loss: 0.092 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.093 - ETA: 3s - loss: 0.093 - ETA: 3s - loss: 0.093 - ETA: 3s - loss: 0.093 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - 4s 245us/step - loss: 0.0948 - val_loss: 0.0870\n",
      "Epoch 16/200\n",
      "17209/17209 [==============================] - ETA: 4s - loss: 0.091 - ETA: 3s - loss: 0.092 - ETA: 3s - loss: 0.092 - ETA: 3s - loss: 0.092 - ETA: 3s - loss: 0.093 - ETA: 3s - loss: 0.093 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - 4s 231us/step - loss: 0.0943 - val_loss: 0.0871\n",
      "Epoch 17/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17209/17209 [==============================] - ETA: 3s - loss: 0.090 - ETA: 3s - loss: 0.093 - ETA: 3s - loss: 0.095 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - 4s 232us/step - loss: 0.0945 - val_loss: 0.0871\n",
      "Epoch 18/200\n",
      "17209/17209 [==============================] - ETA: 3s - loss: 0.095 - ETA: 3s - loss: 0.092 - ETA: 3s - loss: 0.093 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.095 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.095 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - 4s 229us/step - loss: 0.0944 - val_loss: 0.0870\n",
      "Epoch 19/200\n",
      "17209/17209 [==============================] - ETA: 4s - loss: 0.096 - ETA: 3s - loss: 0.095 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - ETA: 0s - loss: 0.094 - 4s 255us/step - loss: 0.0948 - val_loss: 0.0871\n",
      "Epoch 20/200\n",
      "12672/17209 [=====================>........] - ETA: 4s - loss: 0.094 - ETA: 3s - loss: 0.092 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.095 - ETA: 3s - loss: 0.095 - ETA: 3s - loss: 0.095 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.093 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.094 - ETA: 1s - loss: 0.0945"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-b74c50da5eb4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    605\u001b[0m     \u001b[0mx_vl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_vl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mvl_idx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mvl_idx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    606\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 607\u001b[1;33m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_tr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_tr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_vl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_vl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    608\u001b[0m     \u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-b74c50da5eb4>\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(X_train, y_train, X_val, y_val)\u001b[0m\n\u001b[0;32m    591\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mRAdam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwarmup_proportion\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_lr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1e-7\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcrps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    592\u001b[0m     \u001b[0mer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mEarlyStopping\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpatience\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_delta\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1e-4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrestore_best_weights\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmonitor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'val_loss'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 593\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mX_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    594\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    595\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\tf-20-gpu\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1239\u001b[1;33m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[0;32m   1240\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1241\u001b[0m     def evaluate(self,\n",
      "\u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\tf-20-gpu\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[0;32m    199\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 201\u001b[1;33m                 \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'train'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'end'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    202\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m                     \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\tf-20-gpu\\lib\\site-packages\\keras\\callbacks\\callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook\u001b[1;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[0;32m     83\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m             \u001b[0mbatch_hook\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 85\u001b[1;33m             \u001b[0mbatch_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     86\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_delta_ts\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mhook_name\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mt_before_callbacks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\tf-20-gpu\\lib\\site-packages\\keras\\callbacks\\callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m    364\u001b[0m         \"\"\"\n\u001b[0;32m    365\u001b[0m         \u001b[1;31m# For backwards compatibility\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 366\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_batch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    367\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    368\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mon_test_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\tf-20-gpu\\lib\\site-packages\\keras\\callbacks\\callbacks.py\u001b[0m in \u001b[0;36mon_batch_end\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m    601\u001b[0m         \u001b[1;31m# will be handled by on_epoch_end.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    602\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseen\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 603\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_values\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    604\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    605\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\tf-20-gpu\\lib\\site-packages\\keras\\utils\\generic_utils.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, current, values)\u001b[0m\n\u001b[0;32m    450\u001b[0m                 \u001b[0minfo\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;34m'\\n'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    451\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 452\u001b[1;33m             \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    453\u001b[0m             \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    454\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\tf-20-gpu\\lib\\site-packages\\colorama\\ansitowin32.py\u001b[0m in \u001b[0;36mwrite\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__convertor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0misatty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\tf-20-gpu\\lib\\site-packages\\colorama\\ansitowin32.py\u001b[0m in \u001b[0;36mwrite\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    160\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    161\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 162\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite_and_convert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    163\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    164\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrapped\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\tf-20-gpu\\lib\\site-packages\\colorama\\ansitowin32.py\u001b[0m in \u001b[0;36mwrite_and_convert\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    188\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_ansi\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mmatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m             \u001b[0mcursor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 190\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite_plain_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcursor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    191\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\tf-20-gpu\\lib\\site-packages\\colorama\\ansitowin32.py\u001b[0m in \u001b[0;36mwrite_plain_text\u001b[1;34m(self, text, start, end)\u001b[0m\n\u001b[0;32m    194\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mstart\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrapped\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 196\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrapped\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    197\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\tf-20-gpu\\lib\\site-packages\\ipykernel\\iostream.py\u001b[0m in \u001b[0;36mflush\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    349\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpub_thread\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mschedule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    350\u001b[0m                 \u001b[1;31m# and give a timeout to avoid\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 351\u001b[1;33m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mevt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflush_timeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    352\u001b[0m                     \u001b[1;31m# write directly to __stderr__ instead of warning because\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    353\u001b[0m                     \u001b[1;31m# if this is happening sys.stderr may be the problem.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\tf-20-gpu\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    550\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    551\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 552\u001b[1;33m                 \u001b[0msignaled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    553\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    554\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\tf-20-gpu\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    298\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    299\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 300\u001b[1;33m                     \u001b[0mgotit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    301\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    302\u001b[0m                     \u001b[0mgotit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "#from kaggle.competitions import nflrush\n",
    "import tqdm\n",
    "import re\n",
    "from string import punctuation\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import keras\n",
    "from keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, EarlyStopping\n",
    "from keras.utils import plot_model\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "sns.set_style('darkgrid')\n",
    "mpl.rcParams['figure.figsize'] = [15,10]\n",
    "\n",
    "#env = nflrush.make_env()\n",
    "\n",
    "train = pd.read_csv('/input/nfl-big-data-bowl-2020/train.csv', dtype={'WindSpeed': 'object'})\n",
    "\n",
    "# Overall analysis\n",
    "\n",
    "train.head()\n",
    "\n",
    "## Feature Engineering\n",
    "\n",
    "#from https://www.kaggle.com/prashantkikani/nfl-starter-lgb-feature-engg\n",
    "train['DefendersInTheBox_vs_Distance'] = train['DefendersInTheBox'] / train['Distance']\n",
    "\n",
    "# Categorical features\n",
    "\n",
    "cat_features = []\n",
    "for col in train.columns:\n",
    "    if train[col].dtype =='object':\n",
    "        cat_features.append((col, len(train[col].unique())))\n",
    "\n",
    "cat_features\n",
    "\n",
    "#Let's preprocess some of those features.\n",
    "\n",
    "## Stadium Type\n",
    "\n",
    "train['StadiumType'].value_counts()\n",
    "\n",
    "#We already can see some typos, let's fix them.\n",
    "\n",
    "def clean_StadiumType(txt):\n",
    "    if pd.isna(txt):\n",
    "        return np.nan\n",
    "    txt = txt.lower()\n",
    "    txt = ''.join([c for c in txt if c not in punctuation])\n",
    "    txt = re.sub(' +', ' ', txt)\n",
    "    txt = txt.strip()\n",
    "    txt = txt.replace('outside', 'outdoor')\n",
    "    txt = txt.replace('outdor', 'outdoor')\n",
    "    txt = txt.replace('outddors', 'outdoor')\n",
    "    txt = txt.replace('outdoors', 'outdoor')\n",
    "    txt = txt.replace('oudoor', 'outdoor')\n",
    "    txt = txt.replace('indoors', 'indoor')\n",
    "    txt = txt.replace('ourdoor', 'outdoor')\n",
    "    txt = txt.replace('retractable', 'rtr.')\n",
    "    return txt\n",
    "\n",
    "train['StadiumType'] = train['StadiumType'].apply(clean_StadiumType)\n",
    "\n",
    "#By pareto's principle we are just going to focus on the words: outdoor, indoor, closed and open.\n",
    "\n",
    "def transform_StadiumType(txt):\n",
    "    if pd.isna(txt):\n",
    "        return np.nan\n",
    "    if 'outdoor' in txt or 'open' in txt:\n",
    "        return 1\n",
    "    if 'indoor' in txt or 'closed' in txt:\n",
    "        return 0\n",
    "    \n",
    "    return np.nan\n",
    "\n",
    "train['StadiumType'] = train['StadiumType'].apply(transform_StadiumType)\n",
    "\n",
    "## Turf\n",
    "\n",
    "#from https://www.kaggle.com/c/nfl-big-data-bowl-2020/discussion/112681#latest-649087\n",
    "Turf = {'Field Turf':'Artificial', 'A-Turf Titan':'Artificial', 'Grass':'Natural', 'UBU Sports Speed S5-M':'Artificial', \n",
    "        'Artificial':'Artificial', 'DD GrassMaster':'Artificial', 'Natural Grass':'Natural', \n",
    "        'UBU Speed Series-S5-M':'Artificial', 'FieldTurf':'Artificial', 'FieldTurf 360':'Artificial', 'Natural grass':'Natural', 'grass':'Natural', \n",
    "        'Natural':'Natural', 'Artifical':'Artificial', 'FieldTurf360':'Artificial', 'Naturall Grass':'Natural', 'Field turf':'Artificial', \n",
    "        'SISGrass':'Artificial', 'Twenty-Four/Seven Turf':'Artificial', 'natural grass':'Natural'} \n",
    "\n",
    "train['Turf'] = train['Turf'].map(Turf)\n",
    "train['Turf'] = train['Turf'] == 'Natural'\n",
    "\n",
    "## Possession Team\n",
    "\n",
    "train[(train['PossessionTeam']!=train['HomeTeamAbbr']) & (train['PossessionTeam']!=train['VisitorTeamAbbr'])][['PossessionTeam', 'HomeTeamAbbr', 'VisitorTeamAbbr']]\n",
    "\n",
    "#We have some problem with the enconding of the teams such as BLT and BAL or ARZ and ARI.\n",
    "# # \n",
    "# # Let's try to fix them manually.\n",
    "\n",
    "sorted(train['HomeTeamAbbr'].unique()) == sorted(train['VisitorTeamAbbr'].unique())\n",
    "\n",
    "diff_abbr = []\n",
    "for x,y  in zip(sorted(train['HomeTeamAbbr'].unique()), sorted(train['PossessionTeam'].unique())):\n",
    "    if x!=y:\n",
    "        print(x + \" \" + y)\n",
    "\n",
    "#Apparently these are the only three problems, let's fix it.\n",
    "\n",
    "map_abbr = {'ARI': 'ARZ', 'BAL': 'BLT', 'CLE': 'CLV', 'HOU': 'HST'}\n",
    "for abb in train['PossessionTeam'].unique():\n",
    "    map_abbr[abb] = abb\n",
    "\n",
    "train['PossessionTeam'] = train['PossessionTeam'].map(map_abbr)\n",
    "train['HomeTeamAbbr'] = train['HomeTeamAbbr'].map(map_abbr)\n",
    "train['VisitorTeamAbbr'] = train['VisitorTeamAbbr'].map(map_abbr)\n",
    "\n",
    "train['HomePossesion'] = train['PossessionTeam'] == train['HomeTeamAbbr']\n",
    "\n",
    "train['Field_eq_Possession'] = train['FieldPosition'] == train['PossessionTeam']\n",
    "train['HomeField'] = train['FieldPosition'] == train['HomeTeamAbbr']\n",
    "\n",
    "train['HomeField'].value_counts()\n",
    "\n",
    "## Offense formation\n",
    "\n",
    "off_form = train['OffenseFormation'].unique()\n",
    "train['OffenseFormation'].value_counts()\n",
    "\n",
    "#Since I don't have any knowledge about formations, I am just goig to one-hot encode this feature\n",
    "\n",
    "train = pd.concat([train.drop(['OffenseFormation'], axis=1), pd.get_dummies(train['OffenseFormation'], prefix='Formation')], axis=1)\n",
    "dummy_col = train.columns\n",
    "\n",
    "## Game Clock\n",
    "\n",
    "#Game clock is supposed to be a numerical feature.\n",
    "\n",
    "train['GameClock'].value_counts()\n",
    "\n",
    "#Since we already have the quarter feature, we can just divide the Game Clock by 15 minutes so we can get the normalized time left in the quarter.\n",
    "\n",
    "def strtoseconds(txt):\n",
    "    txt = txt.split(':')\n",
    "    ans = int(txt[0])*60 + int(txt[1]) + int(txt[2])/60\n",
    "    return ans\n",
    "\n",
    "train['GameClock'] = train['GameClock'].apply(strtoseconds)\n",
    "\n",
    "sns.distplot(train['GameClock'])\n",
    "\n",
    "## Player height\n",
    "\n",
    "train['PlayerHeight']\n",
    "\n",
    "#We know that 1ft=12in, thus:\n",
    "\n",
    "train['PlayerHeight'] = train['PlayerHeight'].apply(lambda x: 12*int(x.split('-')[0])+int(x.split('-')[1]))\n",
    "\n",
    "train['PlayerBMI'] = 703*(train['PlayerWeight']/(train['PlayerHeight'])**2)\n",
    "\n",
    "## Time handoff and snap and Player BirthDate\n",
    "\n",
    "train['TimeHandoff']\n",
    "\n",
    "train['TimeHandoff'] = train['TimeHandoff'].apply(lambda x: datetime.datetime.strptime(x, \"%Y-%m-%dT%H:%M:%S.%fZ\"))\n",
    "train['TimeSnap'] = train['TimeSnap'].apply(lambda x: datetime.datetime.strptime(x, \"%Y-%m-%dT%H:%M:%S.%fZ\"))\n",
    "\n",
    "train['TimeDelta'] = train.apply(lambda row: (row['TimeHandoff'] - row['TimeSnap']).total_seconds(), axis=1)\n",
    "\n",
    "train['PlayerBirthDate'] = train['PlayerBirthDate'].apply(lambda x: datetime.datetime.strptime(x, \"%m/%d/%Y\"))\n",
    "\n",
    "#Let's use the time handoff to calculate the players age\n",
    "\n",
    "seconds_in_year = 60*60*24*365.25\n",
    "train['PlayerAge'] = train.apply(lambda row: (row['TimeHandoff']-row['PlayerBirthDate']).total_seconds()/seconds_in_year, axis=1)\n",
    "\n",
    "train = train.drop(['TimeHandoff', 'TimeSnap', 'PlayerBirthDate'], axis=1)\n",
    "\n",
    "## Wind Speed and Direction\n",
    "\n",
    "train['WindSpeed'].value_counts()\n",
    "\n",
    "#We can see there are some values that are not standardized(e.g. 12mph), we are going to remove mph from all our values.\n",
    "\n",
    "train['WindSpeed'] = train['WindSpeed'].apply(lambda x: x.lower().replace('mph', '').strip() if not pd.isna(x) else x)\n",
    "\n",
    "train['WindSpeed'].value_counts()\n",
    "\n",
    "#let's replace the ones that has x-y by (x+y)/2\n",
    "# and also the ones with x gusts up to y\n",
    "train['WindSpeed'] = train['WindSpeed'].apply(lambda x: (int(x.split('-')[0])+int(x.split('-')[1]))/2 if not pd.isna(x) and '-' in x else x)\n",
    "train['WindSpeed'] = train['WindSpeed'].apply(lambda x: (int(x.split()[0])+int(x.split()[-1]))/2 if not pd.isna(x) and type(x)!=float and 'gusts up to' in x else x)\n",
    "\n",
    "def str_to_float(txt):\n",
    "    try:\n",
    "        return float(txt)\n",
    "    except:\n",
    "        return -1\n",
    "\n",
    "train['WindSpeed'] = train['WindSpeed'].apply(str_to_float)\n",
    "\n",
    "train['WindDirection'].value_counts()\n",
    "\n",
    "def clean_WindDirection(txt):\n",
    "    if pd.isna(txt):\n",
    "        return np.nan\n",
    "    txt = txt.lower()\n",
    "    txt = ''.join([c for c in txt if c not in punctuation])\n",
    "    txt = txt.replace('from', '')\n",
    "    txt = txt.replace(' ', '')\n",
    "    txt = txt.replace('north', 'n')\n",
    "    txt = txt.replace('south', 's')\n",
    "    txt = txt.replace('west', 'w')\n",
    "    txt = txt.replace('east', 'e')\n",
    "    return txt\n",
    "\n",
    "train['WindDirection'] = train['WindDirection'].apply(clean_WindDirection)\n",
    "\n",
    "train['WindDirection'].value_counts()\n",
    "\n",
    "def transform_WindDirection(txt):\n",
    "    if pd.isna(txt):\n",
    "        return np.nan\n",
    "    \n",
    "    if txt=='n':\n",
    "        return 0\n",
    "    if txt=='nne' or txt=='nen':\n",
    "        return 1/8\n",
    "    if txt=='ne':\n",
    "        return 2/8\n",
    "    if txt=='ene' or txt=='nee':\n",
    "        return 3/8\n",
    "    if txt=='e':\n",
    "        return 4/8\n",
    "    if txt=='ese' or txt=='see':\n",
    "        return 5/8\n",
    "    if txt=='se':\n",
    "        return 6/8\n",
    "    if txt=='ses' or txt=='sse':\n",
    "        return 7/8\n",
    "    if txt=='s':\n",
    "        return 8/8\n",
    "    if txt=='ssw' or txt=='sws':\n",
    "        return 9/8\n",
    "    if txt=='sw':\n",
    "        return 10/8\n",
    "    if txt=='sww' or txt=='wsw':\n",
    "        return 11/8\n",
    "    if txt=='w':\n",
    "        return 12/8\n",
    "    if txt=='wnw' or txt=='nww':\n",
    "        return 13/8\n",
    "    if txt=='nw':\n",
    "        return 14/8\n",
    "    if txt=='nwn' or txt=='nnw':\n",
    "        return 15/8\n",
    "    return np.nan\n",
    "\n",
    "train['WindDirection'] = train['WindDirection'].apply(transform_WindDirection)\n",
    "\n",
    "## PlayDirection\n",
    "\n",
    "train['PlayDirection'].value_counts()\n",
    "\n",
    "train['PlayDirection'] = train['PlayDirection'].apply(lambda x: x.strip() == 'right')\n",
    "\n",
    "## Team\n",
    "\n",
    "train['Team'] = train['Team'].apply(lambda x: x.strip()=='home')\n",
    "\n",
    "## Game Weather\n",
    "\n",
    "train['GameWeather'].unique()\n",
    "\n",
    "#We are going to apply the following preprocessing:\n",
    "# #  \n",
    "# # - Lower case\n",
    "# # - N/A Indoor, N/A (Indoors) and Indoor => indoor Let's try to cluster those together.\n",
    "# # - coudy and clouidy => cloudy\n",
    "# # - party => partly\n",
    "# # - sunny and clear => clear and sunny\n",
    "# # - skies and mostly => \"\"\n",
    "\n",
    "train['GameWeather'] = train['GameWeather'].str.lower()\n",
    "indoor = \"indoor\"\n",
    "train['GameWeather'] = train['GameWeather'].apply(lambda x: indoor if not pd.isna(x) and indoor in x else x)\n",
    "train['GameWeather'] = train['GameWeather'].apply(lambda x: x.replace('coudy', 'cloudy').replace('clouidy', 'cloudy').replace('party', 'partly') if not pd.isna(x) else x)\n",
    "train['GameWeather'] = train['GameWeather'].apply(lambda x: x.replace('clear and sunny', 'sunny and clear') if not pd.isna(x) else x)\n",
    "train['GameWeather'] = train['GameWeather'].apply(lambda x: x.replace('skies', '').replace(\"mostly\", \"\").strip() if not pd.isna(x) else x)\n",
    "\n",
    "train['GameWeather'].unique()\n",
    "\n",
    "#Let's now look at the most common words we have in the weather description\n",
    "\n",
    "from collections import Counter\n",
    "weather_count = Counter()\n",
    "for weather in train['GameWeather']:\n",
    "    if pd.isna(weather):\n",
    "        continue\n",
    "    for word in weather.split():\n",
    "        weather_count[word]+=1\n",
    "        \n",
    "weather_count.most_common()[:15]\n",
    "\n",
    "#To encode our weather we are going to do the following map:\n",
    "# #  \n",
    "# # - climate controlled or indoor => 3, sunny or sun => 2, clear => 1, cloudy => -1, rain => -2, snow => -3, others => 0\n",
    "# # - partly => multiply by 0.5\n",
    "# # \n",
    "# # I don't have any expercience with american football so I don't know if playing in a climate controlled or indoor stadium is good or not, if someone has a good idea on how to encode this it would be nice to leave it in the comments :)\n",
    "\n",
    "def map_weather(txt):\n",
    "    ans = 1\n",
    "    if pd.isna(txt):\n",
    "        return 0\n",
    "    if 'partly' in txt:\n",
    "        ans*=0.5\n",
    "    if 'climate controlled' in txt or 'indoor' in txt:\n",
    "        return ans*3\n",
    "    if 'sunny' in txt or 'sun' in txt:\n",
    "        return ans*2\n",
    "    if 'clear' in txt:\n",
    "        return ans\n",
    "    if 'cloudy' in txt:\n",
    "        return -ans\n",
    "    if 'rain' in txt or 'rainy' in txt:\n",
    "        return -2*ans\n",
    "    if 'snow' in txt:\n",
    "        return -3*ans\n",
    "    return 0\n",
    "\n",
    "train['GameWeather'] = train['GameWeather'].apply(map_weather)\n",
    "\n",
    "## NflId NflIdRusher\n",
    "\n",
    "train['IsRusher'] = train['NflId'] == train['NflIdRusher']\n",
    "\n",
    "train.drop(['NflId', 'NflIdRusher'], axis=1, inplace=True)\n",
    "\n",
    "## PlayDirection problems\n",
    "\n",
    "#As we can see, we have a problem if some features such as X and Y because of the play direction, let's fix those issues\n",
    "\n",
    "### X, orientation and direction\n",
    "\n",
    "train['X'] = train.apply(lambda row: row['X'] if row['PlayDirection'] else 120-row['X'], axis=1)\n",
    "\n",
    "#from https://www.kaggle.com/scirpus/hybrid-gp-and-nn\n",
    "def new_orientation(angle, play_direction):\n",
    "    if play_direction == 0:\n",
    "        new_angle = 360.0 - angle\n",
    "        if new_angle == 360.0:\n",
    "            new_angle = 0.0\n",
    "        return new_angle\n",
    "    else:\n",
    "        return angle\n",
    "    \n",
    "train['Orientation'] = train.apply(lambda row: new_orientation(row['Orientation'], row['PlayDirection']), axis=1)\n",
    "train['Dir'] = train.apply(lambda row: new_orientation(row['Dir'], row['PlayDirection']), axis=1)\n",
    "\n",
    "## YardsLeft\n",
    "# # \n",
    "# # Let's compute how many yards are left to the end-zone.\n",
    "\n",
    "train['YardsLeft'] = train.apply(lambda row: 100-row['YardLine'] if row['HomeField'] else row['YardLine'], axis=1)\n",
    "train['YardsLeft'] = train.apply(lambda row: row['YardsLeft'] if row['PlayDirection'] else 100-row['YardsLeft'], axis=1)\n",
    "\n",
    "((train['YardsLeft']<train['Yards']) | (train['YardsLeft']-100>train['Yards'])).mean()\n",
    "\n",
    "#Clearly:\n",
    "# # Yards<=YardsLeft and YardsLeft-100<=Yards, thus we are going to drop those wrong lines.\n",
    "\n",
    "train.drop(train.index[(train['YardsLeft']<train['Yards']) | (train['YardsLeft']-100>train['Yards'])], inplace=True)\n",
    "\n",
    "# Baseline model\n",
    "\n",
    "#Let's drop the categorical features and run a simple random forest in our model\n",
    "\n",
    "train = train.sort_values(by=['PlayId', 'Team', 'IsRusher', 'JerseyNumber']).reset_index()\n",
    "\n",
    "train.drop(['GameId', 'PlayId', 'index', 'IsRusher', 'Team'], axis=1, inplace=True)\n",
    "\n",
    "cat_features = []\n",
    "for col in train.columns:\n",
    "    if train[col].dtype =='object':\n",
    "        cat_features.append(col)\n",
    "        \n",
    "train = train.drop(cat_features, axis=1)\n",
    "\n",
    "#We are now going to make one big row for each play where the rusher is the last one\n",
    "\n",
    "train.fillna(-999, inplace=True)\n",
    "\n",
    "pd.set_option('max_columns', 100)\n",
    "train.head(100)\n",
    "\n",
    "players_col = []\n",
    "for col in train.columns:\n",
    "    if train[col][:22].std()!=0:\n",
    "        players_col.append(col)\n",
    "\n",
    "X_train = np.array(train[players_col]).reshape(-1, len(players_col)*22)\n",
    "\n",
    "play_col = train.drop(players_col+['Yards'], axis=1).columns\n",
    "X_play_col = np.zeros(shape=(X_train.shape[0], len(play_col)))\n",
    "for i, col in enumerate(play_col):\n",
    "    X_play_col[:, i] = train[col][::22]\n",
    "\n",
    "X_train = np.concatenate([X_train, X_play_col], axis=1)\n",
    "y_train = np.zeros(shape=(X_train.shape[0], 199))\n",
    "for i, yard in enumerate(train['Yards'][::22]):\n",
    "    y_train[i, yard+99:] = np.ones(shape=(1, 100-yard))\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "\n",
    "batch_size=64\n",
    "\n",
    "class RAdam(keras.optimizers.Optimizer):\n",
    "    \"\"\"RAdam optimizer.\n",
    "    # Arguments\n",
    "        learning_rate: float >= 0. Learning rate.\n",
    "        beta_1: float, 0 < beta < 1. Generally close to 1.\n",
    "        beta_2: float, 0 < beta < 1. Generally close to 1.\n",
    "        epsilon: float >= 0. Fuzz factor. If `None`, defaults to `K.epsilon()`.\n",
    "        decay: float >= 0. Learning rate decay over each update.\n",
    "        weight_decay: float >= 0. Weight decay for each param.\n",
    "        amsgrad: boolean. Whether to apply the AMSGrad variant of this\n",
    "            algorithm from the paper \"On the Convergence of Adam and\n",
    "            Beyond\".\n",
    "        total_steps: int >= 0. Total number of training steps. Enable warmup by setting a positive value.\n",
    "        warmup_proportion: 0 < warmup_proportion < 1. The proportion of increasing steps.\n",
    "        min_lr: float >= 0. Minimum learning rate after warmup.\n",
    "    # References\n",
    "        - [Adam - A Method for Stochastic Optimization](https://arxiv.org/abs/1412.6980v8)\n",
    "        - [On the Convergence of Adam and Beyond](https://openreview.net/forum?id=ryQu7f-RZ)\n",
    "        - [On The Variance Of The Adaptive Learning Rate And Beyond](https://arxiv.org/pdf/1908.03265v1.pdf)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, learning_rate=0.001, beta_1=0.9, beta_2=0.999,\n",
    "                 epsilon=None, decay=0., weight_decay=0., amsgrad=False,\n",
    "                 total_steps=0, warmup_proportion=0.1, min_lr=0., **kwargs):\n",
    "        learning_rate = kwargs.pop('lr', learning_rate)\n",
    "        super(RAdam, self).__init__(**kwargs)\n",
    "        with K.name_scope(self.__class__.__name__):\n",
    "            self.iterations = K.variable(0, dtype='int64', name='iterations')\n",
    "            self.learning_rate = K.variable(learning_rate, name='learning_rate')\n",
    "            self.beta_1 = K.variable(beta_1, name='beta_1')\n",
    "            self.beta_2 = K.variable(beta_2, name='beta_2')\n",
    "            self.decay = K.variable(decay, name='decay')\n",
    "            self.weight_decay = K.variable(weight_decay, name='weight_decay')\n",
    "            self.total_steps = K.variable(total_steps, name='total_steps')\n",
    "            self.warmup_proportion = K.variable(warmup_proportion, name='warmup_proportion')\n",
    "            self.min_lr = K.variable(min_lr, name='min_lr')\n",
    "        if epsilon is None:\n",
    "            epsilon = K.epsilon()\n",
    "        self.epsilon = epsilon\n",
    "        self.initial_decay = decay\n",
    "        self.initial_weight_decay = weight_decay\n",
    "        self.initial_total_steps = total_steps\n",
    "        self.amsgrad = amsgrad\n",
    "\n",
    "    def get_updates(self, loss, params):\n",
    "        grads = self.get_gradients(loss, params)\n",
    "        self.updates = [K.update_add(self.iterations, 1)]\n",
    "\n",
    "        lr = self.lr\n",
    "\n",
    "        if self.initial_decay > 0:\n",
    "            lr = lr * (1. / (1. + self.decay * K.cast(self.iterations, K.dtype(self.decay))))\n",
    "\n",
    "        t = K.cast(self.iterations, K.floatx()) + 1\n",
    "\n",
    "        if self.initial_total_steps > 0:\n",
    "            warmup_steps = self.total_steps * self.warmup_proportion\n",
    "            decay_steps = K.maximum(self.total_steps - warmup_steps, 1)\n",
    "            decay_rate = (self.min_lr - lr) / decay_steps\n",
    "            lr = K.switch(\n",
    "                t <= warmup_steps,\n",
    "                lr * (t / warmup_steps),\n",
    "                lr + decay_rate * K.minimum(t - warmup_steps, decay_steps),\n",
    "            )\n",
    "\n",
    "        ms = [K.zeros(K.int_shape(p), dtype=K.dtype(p), name='m_' + str(i)) for (i, p) in enumerate(params)]\n",
    "        vs = [K.zeros(K.int_shape(p), dtype=K.dtype(p), name='v_' + str(i)) for (i, p) in enumerate(params)]\n",
    "\n",
    "        if self.amsgrad:\n",
    "            vhats = [K.zeros(K.int_shape(p), dtype=K.dtype(p), name='vhat_' + str(i)) for (i, p) in enumerate(params)]\n",
    "        else:\n",
    "            vhats = [K.zeros(1, name='vhat_' + str(i)) for i in range(len(params))]\n",
    "\n",
    "        self.weights = [self.iterations] + ms + vs + vhats\n",
    "\n",
    "        beta_1_t = K.pow(self.beta_1, t)\n",
    "        beta_2_t = K.pow(self.beta_2, t)\n",
    "\n",
    "        sma_inf = 2.0 / (1.0 - self.beta_2) - 1.0\n",
    "        sma_t = sma_inf - 2.0 * t * beta_2_t / (1.0 - beta_2_t)\n",
    "\n",
    "        for p, g, m, v, vhat in zip(params, grads, ms, vs, vhats):\n",
    "            m_t = (self.beta_1 * m) + (1. - self.beta_1) * g\n",
    "            v_t = (self.beta_2 * v) + (1. - self.beta_2) * K.square(g)\n",
    "\n",
    "            m_corr_t = m_t / (1.0 - beta_1_t)\n",
    "            if self.amsgrad:\n",
    "                vhat_t = K.maximum(vhat, v_t)\n",
    "                v_corr_t = K.sqrt(vhat_t / (1.0 - beta_2_t))\n",
    "                self.updates.append(K.update(vhat, vhat_t))\n",
    "            else:\n",
    "                v_corr_t = K.sqrt(v_t / (1.0 - beta_2_t))\n",
    "\n",
    "            r_t = K.sqrt((sma_t - 4.0) / (sma_inf - 4.0) *\n",
    "                         (sma_t - 2.0) / (sma_inf - 2.0) *\n",
    "                         sma_inf / sma_t)\n",
    "\n",
    "            p_t = K.switch(sma_t >= 5, r_t * m_corr_t / (v_corr_t + self.epsilon), m_corr_t)\n",
    "\n",
    "            if self.initial_weight_decay > 0:\n",
    "                p_t += self.weight_decay * p\n",
    "\n",
    "            p_t = p - lr * p_t\n",
    "\n",
    "            self.updates.append(K.update(m, m_t))\n",
    "            self.updates.append(K.update(v, v_t))\n",
    "            new_p = p_t\n",
    "\n",
    "            # Apply constraints.\n",
    "            if getattr(p, 'constraint', None) is not None:\n",
    "                new_p = p.constraint(new_p)\n",
    "\n",
    "            self.updates.append(K.update(p, new_p))\n",
    "        return self.updates\n",
    "\n",
    "    @property\n",
    "    def lr(self):\n",
    "        return self.learning_rate\n",
    "\n",
    "    @lr.setter\n",
    "    def lr(self, learning_rate):\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            'learning_rate': float(K.get_value(self.learning_rate)),\n",
    "            'beta_1': float(K.get_value(self.beta_1)),\n",
    "            'beta_2': float(K.get_value(self.beta_2)),\n",
    "            'decay': float(K.get_value(self.decay)),\n",
    "            'weight_decay': float(K.get_value(self.weight_decay)),\n",
    "            'epsilon': self.epsilon,\n",
    "            'amsgrad': self.amsgrad,\n",
    "            'total_steps': float(K.get_value(self.total_steps)),\n",
    "            'warmup_proportion': float(K.get_value(self.warmup_proportion)),\n",
    "            'min_lr': float(K.get_value(self.min_lr)),\n",
    "        }\n",
    "        base_config = super(RAdam, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "#from https://www.kaggle.com/davidcairuz/nfl-neural-network-w-softmax\n",
    "def crps(y_true, y_pred):\n",
    "    return K.mean(K.square(y_true - K.cumsum(y_pred, axis=1)), axis=1)\n",
    "\n",
    "def get_model():\n",
    "    x = keras.layers.Input(shape=[X_train.shape[1]])\n",
    "    fc1 = keras.layers.Dense(units=450, input_shape=[X_train.shape[1]])(x)\n",
    "    act1 = keras.layers.PReLU()(fc1)\n",
    "    bn1 = keras.layers.BatchNormalization()(act1)\n",
    "    dp1 = keras.layers.Dropout(0.55)(bn1)\n",
    "    concat1 = keras.layers.Concatenate()([x, dp1])\n",
    "    fc2 = keras.layers.Dense(units=600)(concat1)\n",
    "    act2 = keras.layers.PReLU()(fc2)\n",
    "    bn2 = keras.layers.BatchNormalization()(act2)\n",
    "    dp2 = keras.layers.Dropout(0.55)(bn2)\n",
    "    concat2 = keras.layers.Concatenate()([concat1, dp2])\n",
    "    fc3 = keras.layers.Dense(units=400)(concat2)\n",
    "    act3 = keras.layers.PReLU()(fc3)\n",
    "    bn3 = keras.layers.BatchNormalization()(act3)\n",
    "    dp3 = keras.layers.Dropout(0.55)(bn3)\n",
    "    concat3 = keras.layers.Concatenate([concat2, dp3])\n",
    "    output = keras.layers.Dense(units=199, activation='softmax')(concat2)\n",
    "    model = keras.models.Model(inputs=[x], outputs=[output])\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_model(X_train, y_train, X_val, y_val):\n",
    "    model = get_model()\n",
    "    model.compile(optimizer=RAdam(warmup_proportion=0.1, min_lr=1e-7), loss=crps)\n",
    "    er = EarlyStopping(patience=20, min_delta=1e-4, restore_best_weights=True, monitor='val_loss')\n",
    "    model.fit(X_train, y_train, epochs=200, callbacks=[er], validation_data=[X_val, y_val], batch_size=batch_size)\n",
    "    return model\n",
    "\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "\n",
    "rkf = RepeatedKFold(n_splits=4, n_repeats=6)\n",
    "\n",
    "models = []\n",
    "\n",
    "for tr_idx, vl_idx in rkf.split(X_train, y_train):\n",
    "    \n",
    "    x_tr, y_tr = X_train[tr_idx], y_train[tr_idx]\n",
    "    x_vl, y_vl = X_train[vl_idx], y_train[vl_idx]\n",
    "    \n",
    "    model = train_model(x_tr, y_tr, x_vl, y_vl)\n",
    "    models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_cell_guid": "ab9b10a8-e9ca-4864-86bb-21450d2b5cf2",
    "_uuid": "5262594e-a1f3-4dbb-b0de-63185905f9c5"
   },
   "outputs": [],
   "source": [
    "def make_pred(df, sample, env, models):\n",
    "    df['StadiumType'] = df['StadiumType'].apply(clean_StadiumType)\n",
    "    df['StadiumType'] = df['StadiumType'].apply(transform_StadiumType)\n",
    "    df['DefendersInTheBox_vs_Distance'] = df['DefendersInTheBox'] / df['Distance']\n",
    "    df['OffenseFormation'] = df['OffenseFormation'].apply(lambda x: x if x in off_form else np.nan)\n",
    "    df = pd.concat([df.drop(['OffenseFormation'], axis=1), pd.get_dummies(df['OffenseFormation'], prefix='Formation')], axis=1)\n",
    "    missing_cols = set( dummy_col ) - set( df.columns )-set('Yards')\n",
    "    for c in missing_cols:\n",
    "        df[c] = 0\n",
    "    df = df[dummy_col]\n",
    "    df.drop(['Yards'], axis=1, inplace=True)\n",
    "    df['Turf'] = df['Turf'].map(Turf)\n",
    "    df['Turf'] = df['Turf'] == 'Natural'\n",
    "    df['PossessionTeam'] = df['PossessionTeam'].map(map_abbr)\n",
    "    df['HomeTeamAbbr'] = df['HomeTeamAbbr'].map(map_abbr)\n",
    "    df['VisitorTeamAbbr'] = df['VisitorTeamAbbr'].map(map_abbr)\n",
    "    df['HomePossesion'] = df['PossessionTeam'] == df['HomeTeamAbbr']\n",
    "    df['Field_eq_Possession'] = df['FieldPosition'] == df['PossessionTeam']\n",
    "    df['HomeField'] = df['FieldPosition'] == df['HomeTeamAbbr']\n",
    "    df['GameClock'] = df['GameClock'].apply(strtoseconds)\n",
    "    df['PlayerHeight'] = df['PlayerHeight'].apply(lambda x: 12*int(x.split('-')[0])+int(x.split('-')[1]))\n",
    "    df['PlayerBMI'] = 703*(df['PlayerWeight']/(df['PlayerHeight'])**2)\n",
    "    df['TimeHandoff'] = df['TimeHandoff'].apply(lambda x: datetime.datetime.strptime(x, \"%Y-%m-%dT%H:%M:%S.%fZ\"))\n",
    "    df['TimeSnap'] = df['TimeSnap'].apply(lambda x: datetime.datetime.strptime(x, \"%Y-%m-%dT%H:%M:%S.%fZ\"))\n",
    "    df['TimeDelta'] = df.apply(lambda row: (row['TimeHandoff'] - row['TimeSnap']).total_seconds(), axis=1)\n",
    "    df['PlayerBirthDate'] = df['PlayerBirthDate'].apply(lambda x: datetime.datetime.strptime(x, \"%m/%d/%Y\"))\n",
    "    seconds_in_year = 60*60*24*365.25\n",
    "    df['PlayerAge'] = df.apply(lambda row: (row['TimeHandoff']-row['PlayerBirthDate']).total_seconds()/seconds_in_year, axis=1)\n",
    "    df['WindSpeed'] = df['WindSpeed'].apply(lambda x: x.lower().replace('mph', '').strip() if not pd.isna(x) else x)\n",
    "    df['WindSpeed'] = df['WindSpeed'].apply(lambda x: (int(x.split('-')[0])+int(x.split('-')[1]))/2 if not pd.isna(x) and '-' in x else x)\n",
    "    df['WindSpeed'] = df['WindSpeed'].apply(lambda x: (int(x.split()[0])+int(x.split()[-1]))/2 if not pd.isna(x) and type(x)!=float and 'gusts up to' in x else x)\n",
    "    df['WindSpeed'] = df['WindSpeed'].apply(str_to_float)\n",
    "    df['WindDirection'] = df['WindDirection'].apply(clean_WindDirection)\n",
    "    df['WindDirection'] = df['WindDirection'].apply(transform_WindDirection)\n",
    "    df['PlayDirection'] = df['PlayDirection'].apply(lambda x: x.strip() == 'right')\n",
    "    df['Team'] = df['Team'].apply(lambda x: x.strip()=='home')\n",
    "    indoor = \"indoor\"\n",
    "    df['GameWeather'] = df['GameWeather'].apply(lambda x: indoor if not pd.isna(x) and indoor in x else x)\n",
    "    df['GameWeather'] = df['GameWeather'].apply(lambda x: x.lower().replace('coudy', 'cloudy').replace('clouidy', 'cloudy').replace('party', 'partly').replace('clear and sunny', 'sunny and clear').replace('skies', '').replace(\"mostly\", \"\").strip() if not pd.isna(x) else x)\n",
    "    df['GameWeather'] = df['GameWeather'].apply(map_weather)\n",
    "    df['IsRusher'] = df['NflId'] == df['NflIdRusher']\n",
    "    df['X'] = df.apply(lambda row: row['X'] if row['PlayDirection'] else 120-row['X'], axis=1)\n",
    "    df['Orientation'] = df.apply(lambda row: new_orientation(row['Orientation'], row['PlayDirection']), axis=1)\n",
    "    df['Dir'] = df.apply(lambda row: new_orientation(row['Dir'], row['PlayDirection']), axis=1)\n",
    "    df['YardsLeft'] = df.apply(lambda row: 100-row['YardLine'] if row['HomeField'] else row['YardLine'], axis=1)\n",
    "    df['YardsLeft'] = df.apply(lambda row: row['YardsLeft'] if row['PlayDirection'] else 100-row['YardsLeft'], axis=1)\n",
    "    df = df.sort_values(by=['PlayId', 'Team', 'IsRusher', 'JerseyNumber']).reset_index()\n",
    "    df = df.drop(['TimeHandoff', 'TimeSnap', 'PlayerBirthDate', 'NflId', 'NflIdRusher', 'GameId', 'PlayId', 'index', 'IsRusher', 'Team'], axis=1)\n",
    "    cat_features = []\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype =='object':\n",
    "            cat_features.append(col)\n",
    "\n",
    "    df = df.drop(cat_features, axis=1)\n",
    "    df.fillna(-999, inplace=True)\n",
    "    X = np.array(df[players_col]).reshape(-1, len(players_col)*22)\n",
    "    play_col = df.drop(players_col, axis=1).columns\n",
    "    X_play_col = np.zeros(shape=(X.shape[0], len(play_col)))\n",
    "    for i, col in enumerate(play_col):\n",
    "        X_play_col[:, i] = df[col][::22]\n",
    "    X = np.concatenate([X, X_play_col], axis=1)\n",
    "    X = scaler.transform(X)\n",
    "    y_pred = np.mean([np.cumsum(model.predict(X), axis=1) for model in models], axis=0)\n",
    "    yardsleft = np.array(df['YardsLeft'][::22])\n",
    "    \n",
    "    for i in range(len(yardsleft)):\n",
    "        y_pred[i, :yardsleft[i]-1] = 0\n",
    "        y_pred[i, yardsleft[i]+100:] = 1\n",
    "    env.predict(pd.DataFrame(data=y_pred.clip(0,1),columns=sample.columns))\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_cell_guid": "93aeb16c-62c7-41d6-9bb3-f1b3221ff6d0",
    "_uuid": "fd961406-0dc1-46c5-8848-305cc3215914"
   },
   "outputs": [],
   "source": [
    "for test, sample in tqdm.tqdm(env.iter_test()):\n",
    "     make_pred(test, sample, env, models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_cell_guid": "677f994f-93b1-49d1-8ce4-b37795b70535",
    "_uuid": "0714fffd-94b2-4961-b373-9557fb096f03"
   },
   "outputs": [],
   "source": [
    "env.write_submission_file()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "c2ab4c1b-7697-4858-8d88-7cf34c880028",
    "_uuid": "0a9fef85-4aba-43c3-a23f-6f3b4daec11c"
   },
   "source": [
    "# End\n",
    "# # \n",
    "# # If you reached this far please comment and upvote this kernel, feel free to make improvements on the kernel and please share if you found anything useful!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
