{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.losses import binary_crossentropy\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH = '/input/understanding_cloud_organization/'\n",
    "TRAIN_IMAGE = '/input/understanding_cloud_organization/train_images'\n",
    "TEST_IMAGE = '/input/understanding_cloud_organization/test_images'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_image = ['046586a.jpg', '1588d4c.jpg', '1e40a05.jpg', '41f92e5.jpg', '449b792.jpg',\n",
    "             '563fc48.jpg', '8bd81ce.jpg', 'c0306e5.jpg', 'c26c635.jpg', 'e04fea3.jpg',\n",
    "             'e5f2f24.jpg', 'eda52f2.jpg', 'fa645da.jpg']\n",
    "train_df = pd.read_csv(os.path.join(BASE_PATH, 'train.csv'))\n",
    "train_df['Image'] = train_df['Image_Label'].apply(lambda x: x.split('_')[0])\n",
    "train_df['Label'] = train_df['Image_Label'].apply(lambda x: x.split('_')[1])\n",
    "train_df['has_mask'] = ~train_df['EncodedPixels'].isna()\n",
    "train_df=train_df[~train_df['Image'].isin(bad_image)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_count_df = train_df.groupby('Image').agg(np.sum).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility fuctions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#post process by threshold and mini_mask, with rectangle funtion(optional)\n",
    "def post_process(mask, threshold, min_size, input_size=(350, 525), image=None, rectangle=False):\n",
    "    new_mask = np.zeros(input_size, dtype='uint8')\n",
    "    blk_mask = np.empty(input_size, dtype='uint8')\n",
    "    mask = cv2.threshold(mask, threshold, 1, cv2.THRESH_BINARY)[1] #threshold\n",
    "    \n",
    "    num_component, component = cv2.connectedComponents(mask.astype(np.uint8)) #component split\n",
    "\n",
    "    predictions = np.zeros(input_size, np.float32)\n",
    "    num = 0\n",
    "    \n",
    "    for c in range(1, num_component):\n",
    "        p = (component == c)\n",
    "        #remove mask component if size < mini_size\n",
    "        if p.sum() > min_size:\n",
    "            #if apply, transform mask shape to rectanle\n",
    "            if rectangle:\n",
    "                new_mask = rectangle_process(image, new_mask, p)\n",
    "            predictions[p] = 1\n",
    "            num += 1\n",
    "    return predictions, new_mask, num\n",
    "\n",
    "def rectangle_process(img, new_mask, block):\n",
    "    cnts, _ = cv2.findContours(block.astype(np.uint8), cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    for c in cnts:\n",
    "        x, y, w, h = cv2.boundingRect(c)\n",
    "        cv2.rectangle(new_mask, (x, y), (x+w, y+h), 1, -1)\n",
    "        l = np.array([0, 0, 0])\n",
    "        u = np.array([0, 0, 0])\n",
    "        blk_mask = cv2.inRange(img, l, u)\n",
    "        new_mask = cv2.subtract(new_mask, blk_mask)\n",
    "        return new_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def np_resize(img, input_shape):\n",
    "    height, width = input_shape\n",
    "    return cv2.resize(img, (width, height))\n",
    "\n",
    "def mask2rle(img):\n",
    "    '''\n",
    "    img: numpy array, 1 - mask, 0 - background\n",
    "    Returns run length as string formated\n",
    "    '''\n",
    "    pixels= img.T.flatten()\n",
    "    pixels = np.concatenate([[0], pixels, [0]])\n",
    "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n",
    "    runs[1::2] -= runs[::2]\n",
    "    return ' '.join(str(x) for x in runs)\n",
    "\n",
    "def rle2mask(rle, input_shape):\n",
    "    width, height = input_shape[:2]\n",
    "    \n",
    "    mask= np.zeros( width*height ).astype(np.uint8)\n",
    "    \n",
    "    array = np.asarray([int(x) for x in rle.split()])\n",
    "    starts = array[0::2]\n",
    "    lengths = array[1::2]\n",
    "\n",
    "    current_position = 0\n",
    "    for index, start in enumerate(starts):\n",
    "        mask[int(start):int(start+lengths[index])] = 1\n",
    "        current_position += lengths[index]\n",
    "        \n",
    "    return mask.reshape(height, width).T\n",
    "\n",
    "def mask_maker(rles, input_shape, reshape=None):\n",
    "    depth = len(rles)\n",
    "    if reshape is None:\n",
    "        masks = np.zeros((*input_shape, depth))\n",
    "    else:\n",
    "        masks = np.zeros((*reshape, depth))\n",
    "    \n",
    "    for i, rle in enumerate(rles):\n",
    "        if type(rle) is str:\n",
    "            if reshape is None:\n",
    "                masks[:, :, i] = rle2mask(rle, input_shape)\n",
    "            else:\n",
    "                mask = rle2mask(rle, input_shape)\n",
    "                reshaped_mask = np_resize(mask, reshape)\n",
    "                masks[:, :, i] = reshaped_mask\n",
    "    \n",
    "    return masks\n",
    "\n",
    "def build_rles(masks, reshape=None):\n",
    "    width, height, depth = masks.shape\n",
    "    \n",
    "    rles = []\n",
    "    \n",
    "    for i in range(depth):\n",
    "        mask = masks[:, :, i]\n",
    "        \n",
    "        if reshape:\n",
    "            mask = mask.astype(np.float32)\n",
    "            mask = np_resize(mask, reshape).astype(np.int64)\n",
    "        \n",
    "        rle = mask2rle(mask)\n",
    "        rles.append(rle)\n",
    "        \n",
    "    return rles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_coef(y_true, y_pred, smooth=1):\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
    "\n",
    "def dice_loss(y_true, y_pred):\n",
    "    smooth = 1.\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = y_true_f * y_pred_f\n",
    "    score = (2. * K.sum(intersection) + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
    "    return 1. - score\n",
    "\n",
    "def bce_dice_loss(y_true, y_pred):\n",
    "    return binary_crossentropy(y_true, y_pred) + dice_loss(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(keras.utils.Sequence):\n",
    "    def __init__(self, list_IDs, df, target_df, batch_size, mode='fit',\n",
    "                base_path='/input/understanding_cloud_organization/train_images',\n",
    "                channels=3, classes=4, input_size=(1400, 2100) ,augment=False, gamma=None,\n",
    "                reshape=None, random_state=2019, shuffle=True):\n",
    "    \n",
    "        self.list_IDs = list_IDs\n",
    "        self.df = df\n",
    "        self.target_df = target_df\n",
    "        self.batch_size = batch_size\n",
    "        self.mode = mode\n",
    "        self.base_path = base_path\n",
    "        self.channels = channels\n",
    "        self.classes = classes\n",
    "        self.input_size = input_size\n",
    "        self.augment = augment\n",
    "        self.reshape = reshape\n",
    "        self.random_state = random_state\n",
    "        self.shuffle = shuffle\n",
    "        self.gamma = gamma\n",
    "\n",
    "        self.on_epoch_end()\n",
    "        np.random.seed(self.random_state)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        indexes = self.indexes[index * self.batch_size: (index + 1) * self.batch_size]\n",
    "        \n",
    "        list_IDs_batch = [self.list_IDs[k] for k in indexes]\n",
    "        \n",
    "        X = self.__generator_X(list_IDs_batch)\n",
    "        \n",
    "        if self.mode == 'fit':\n",
    "            y = self.__generator_y(list_IDs_batch)\n",
    "            \n",
    "            if self.augment:\n",
    "                X, y = self.__batch_augment(X, y)\n",
    "            \n",
    "            return X, y\n",
    "        \n",
    "        elif self.mode == 'predict':\n",
    "            return X\n",
    "        \n",
    "        else:\n",
    "            raise AttributeError('The mode parameter should be set to \"fit\" or \"predict\".')\n",
    "            \n",
    "    def on_epoch_end(self):\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        if self.shuffle:\n",
    "            np.random.seed(self.random_state)\n",
    "            np.random.shuffle(self.indexes)\n",
    "    \n",
    "    def __generator_X(self, list_IDs_batch):\n",
    "        \n",
    "        if self.reshape is not None:\n",
    "            X = np.empty((self.batch_size, *self.reshape, self.channels))\n",
    "        else:\n",
    "            X = np.empty((self.batch_size, *self.input_size, self.channels))\n",
    "        \n",
    "        for i, ID in enumerate(list_IDs_batch):\n",
    "            fn = self.df['Image'].iloc[ID]\n",
    "            img_path = f'{self.base_path}/{fn}'\n",
    "            img = self.__load_rgb(img_path)\n",
    "            \n",
    "            if self.reshape is not None:\n",
    "                img = np_resize(img, self.reshape)\n",
    "            if self.gamma is not None:\n",
    "                img = adjust_gamma(img, gamma=self.gamma)\n",
    "            \n",
    "            X[i, ] = img\n",
    "            \n",
    "        return X\n",
    "    \n",
    "    def __generator_y(self, list_IDs_batch):\n",
    "        \n",
    "        if self.reshape is not None:\n",
    "            y = np.empty((self.batch_size, *self.reshape, self.classes), dtype=int)\n",
    "        else:\n",
    "            y = np.empty((self.batch_size, *self.input_size, self.classes), dtype=int)\n",
    "        \n",
    "        for i, ID in enumerate(list_IDs_batch):\n",
    "            img = self.df['Image'].iloc[ID]\n",
    "            img_df = self.target_df[self.target_df['Image'] == img]\n",
    "            rles = img_df['EncodedPixels']\n",
    "            \n",
    "            if self.reshape is None:\n",
    "                mask = mask_maker(rles, input_shape=self.input_size)\n",
    "            else:\n",
    "                mask = mask_maker(rles, input_shape=self.input_size, reshape=self.reshape)\n",
    "                        \n",
    "            y[i, ] = mask\n",
    "            \n",
    "        return y\n",
    "            \n",
    "    def __load_grayscale(self, img_path):\n",
    "        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "        img = img.astype(np.float32) / 255.\n",
    "        img = np.expand_dims(img, axis=-1)\n",
    "        return img\n",
    "    \n",
    "    def __load_rgb(self, img_path):\n",
    "        img = cv2.imread(img_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img = img.astype(np.float32) / 255.\n",
    "        return img\n",
    "    \n",
    "    def __random_transform(self, img, masks):\n",
    "        composition = albu.Compose([\n",
    "            albu.HorizontalFlip(),\n",
    "            albu.VerticalFlip(),\n",
    "            albu.ShiftScaleRotate(shift_limit=0.1, rotate_limit=30),\n",
    "            albu.OneOf([\n",
    "                albu.GridDistortion(p=0.5),\n",
    "                albu.OpticalDistortion(p=0.5, distort_limit=2, shift_limit=0.5),\n",
    "            ], p=0.5)\n",
    "        ])\n",
    "        \n",
    "        composed = composition(image=img, mask=masks)\n",
    "        aug_img = composed['image']\n",
    "        aug_mask = composed['mask']\n",
    "        return aug_img, aug_mask\n",
    "    \n",
    "    def __batch_augment(self, img_batch, mask_batch):\n",
    "        for i in range(img_batch.shape[0]):\n",
    "            img_batch[i,], mask_batch[i, ] = self.__random_transform(img_batch[i, ], mask_batch[i, ]) \n",
    "        return img_batch, mask_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read submission csv and test images\n",
    "sub_df = pd.read_csv('/input/understanding_cloud_organization/sample_submission.csv')\n",
    "sub_df['Image'] = sub_df['Image_Label'].apply(lambda x: x.split('_')[0])\n",
    "test_imgs = pd.DataFrame(sub_df['Image'].unique(), columns=['Image'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble trained models\n",
    "Consider we dont have enought computing resourse, therefore I training each model sepratly and ensemble all model's predictions and calculate it for my final predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_models = ['model_DenseNet169_fpn_n2.h5','model_resnet101_unet_sm.h5', \n",
    "              'model_incepv3_fpn_n2.h5', 'model_res34_sm.h5']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running: model_DenseNet169_fpn_n2.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1107 11:55:14.053850  8348 deprecation.py:323] From C:\\Users\\zessi\\AppData\\Local\\anaconda3\\envs\\tf-20-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 23s 235ms/step\n",
      "100/100 [==============================] - 13s 127ms/step\n",
      "100/100 [==============================] - 11s 113ms/step\n",
      "100/100 [==============================] - 11s 113ms/step\n",
      "100/100 [==============================] - 11s 107ms/step\n",
      "100/100 [==============================] - 11s 106ms/step\n",
      "100/100 [==============================] - 11s 106ms/step\n",
      "100/100 [==============================] - 11s 106ms/step\n",
      "100/100 [==============================] - 11s 107ms/step\n",
      "100/100 [==============================] - 11s 106ms/step\n",
      "100/100 [==============================] - 11s 106ms/step\n",
      "100/100 [==============================] - 11s 106ms/step\n",
      "100/100 [==============================] - 11s 106ms/step\n",
      "100/100 [==============================] - 11s 106ms/step\n",
      "100/100 [==============================] - 11s 107ms/step\n",
      "100/100 [==============================] - 11s 106ms/step\n",
      "100/100 [==============================] - 11s 106ms/step\n",
      "100/100 [==============================] - 11s 106ms/step\n",
      "100/100 [==============================] - 11s 108ms/step\n",
      "100/100 [==============================] - 11s 105ms/step\n",
      "100/100 [==============================] - 10s 105ms/step\n",
      "100/100 [==============================] - 11s 105ms/step\n",
      "100/100 [==============================] - 11s 106ms/step\n",
      "100/100 [==============================] - 11s 106ms/step\n",
      "100/100 [==============================] - 11s 106ms/step\n",
      "100/100 [==============================] - 11s 106ms/step\n",
      "100/100 [==============================] - 11s 107ms/step\n",
      "100/100 [==============================] - 11s 107ms/step\n",
      "100/100 [==============================] - 11s 107ms/step\n",
      "100/100 [==============================] - 11s 106ms/step\n",
      "100/100 [==============================] - 11s 107ms/step\n",
      "100/100 [==============================] - 11s 106ms/step\n",
      "100/100 [==============================] - 11s 107ms/step\n",
      "100/100 [==============================] - 11s 108ms/step\n",
      "100/100 [==============================] - 11s 106ms/step\n",
      "100/100 [==============================] - 11s 107ms/step\n",
      "98/98 [==============================] - 10s 104ms/step\n",
      "running: model_resnet101_unet_sm.h5\n",
      "100/100 [==============================] - 16s 165ms/step\n",
      "100/100 [==============================] - 10s 103ms/step\n",
      "100/100 [==============================] - 10s 104ms/step\n",
      "100/100 [==============================] - 10s 104ms/step\n",
      "100/100 [==============================] - 10s 105ms/step\n",
      "100/100 [==============================] - 10s 105ms/step\n",
      "100/100 [==============================] - 10s 105ms/step\n",
      "100/100 [==============================] - 11s 106ms/step\n",
      "100/100 [==============================] - 11s 106ms/step\n",
      "100/100 [==============================] - 10s 103ms/step\n",
      "100/100 [==============================] - 10s 104ms/step\n",
      "100/100 [==============================] - 10s 103ms/step\n",
      "100/100 [==============================] - 10s 104ms/step\n",
      "100/100 [==============================] - 10s 104ms/step\n",
      "100/100 [==============================] - 10s 105ms/step\n",
      "100/100 [==============================] - 10s 105ms/step\n",
      "100/100 [==============================] - 10s 105ms/step\n",
      "100/100 [==============================] - 12s 120ms/step\n",
      "100/100 [==============================] - 10s 105ms/step\n",
      "100/100 [==============================] - 10s 103ms/step\n",
      "100/100 [==============================] - 10s 102ms/step\n",
      "100/100 [==============================] - 10s 105ms/step\n",
      "100/100 [==============================] - 10s 104ms/step\n",
      "100/100 [==============================] - 10s 105ms/step\n",
      "100/100 [==============================] - 11s 106ms/step\n",
      "100/100 [==============================] - 10s 104ms/step\n",
      "100/100 [==============================] - 11s 106ms/step\n",
      "100/100 [==============================] - 10s 104ms/step\n",
      "100/100 [==============================] - 10s 104ms/step\n",
      "100/100 [==============================] - 10s 105ms/step\n",
      "100/100 [==============================] - 10s 105ms/step\n",
      "100/100 [==============================] - 10s 105ms/step\n",
      "100/100 [==============================] - 11s 106ms/step\n",
      "100/100 [==============================] - 10s 104ms/step\n",
      "100/100 [==============================] - 11s 105ms/step\n",
      "100/100 [==============================] - 11s 105ms/step\n",
      "98/98 [==============================] - 10s 104ms/step\n",
      "running: model_incepv3_fpn_n2.h5\n",
      "100/100 [==============================] - 16s 160ms/step\n",
      "100/100 [==============================] - 10s 103ms/step\n",
      "100/100 [==============================] - 10s 104ms/step\n",
      "100/100 [==============================] - 10s 104ms/step\n",
      "100/100 [==============================] - 10s 105ms/step\n",
      "100/100 [==============================] - 11s 108ms/step\n",
      "100/100 [==============================] - 10s 103ms/step\n",
      "100/100 [==============================] - 11s 105ms/step\n",
      "100/100 [==============================] - 10s 103ms/step\n",
      "100/100 [==============================] - 10s 104ms/step\n",
      "100/100 [==============================] - 10s 103ms/step\n",
      "100/100 [==============================] - 11s 107ms/step\n",
      "100/100 [==============================] - 10s 104ms/step\n",
      "100/100 [==============================] - 10s 104ms/step\n",
      "100/100 [==============================] - 11s 106ms/step\n",
      "100/100 [==============================] - 11s 106ms/step\n",
      "100/100 [==============================] - 11s 107ms/step\n",
      "100/100 [==============================] - 10s 104ms/step\n",
      "100/100 [==============================] - 10s 104ms/step\n",
      "100/100 [==============================] - 10s 104ms/step\n",
      "100/100 [==============================] - 11s 109ms/step\n",
      "100/100 [==============================] - 10s 103ms/step\n",
      "100/100 [==============================] - 10s 103ms/step\n",
      "100/100 [==============================] - 10s 104ms/step\n",
      "100/100 [==============================] - 10s 103ms/step\n",
      "100/100 [==============================] - 10s 104ms/step\n",
      "100/100 [==============================] - 10s 105ms/step\n",
      "100/100 [==============================] - 10s 105ms/step\n",
      "100/100 [==============================] - 11s 106ms/step\n",
      "100/100 [==============================] - 10s 105ms/step\n",
      "100/100 [==============================] - 11s 107ms/step\n",
      "100/100 [==============================] - 10s 104ms/step\n",
      "100/100 [==============================] - 11s 108ms/step\n",
      "100/100 [==============================] - 10s 105ms/step\n",
      "100/100 [==============================] - 10s 104ms/step\n",
      "100/100 [==============================] - 10s 104ms/step\n",
      "98/98 [==============================] - 10s 104ms/step\n",
      "running: model_res34_sm.h5\n",
      "100/100 [==============================] - 11s 107ms/step\n",
      "100/100 [==============================] - 9s 92ms/step\n",
      "100/100 [==============================] - 9s 92ms/step\n",
      "100/100 [==============================] - 9s 91ms/step\n",
      "100/100 [==============================] - 9s 92ms/step\n",
      "100/100 [==============================] - 9s 92ms/step\n",
      "100/100 [==============================] - 9s 93ms/step\n",
      "100/100 [==============================] - 9s 91ms/step\n",
      "100/100 [==============================] - 9s 92ms/step\n",
      "100/100 [==============================] - 9s 91ms/step\n",
      "100/100 [==============================] - 9s 91ms/step\n",
      "100/100 [==============================] - 9s 91ms/step\n",
      "100/100 [==============================] - 9s 92ms/step\n",
      "100/100 [==============================] - 9s 93ms/step\n",
      "100/100 [==============================] - 9s 93ms/step\n",
      "100/100 [==============================] - 9s 92ms/step\n",
      "100/100 [==============================] - 9s 91ms/step\n",
      "100/100 [==============================] - 9s 92ms/step\n",
      "100/100 [==============================] - 9s 94ms/step\n",
      "100/100 [==============================] - 9s 91ms/step\n",
      "100/100 [==============================] - 9s 92ms/step\n",
      "100/100 [==============================] - 9s 91ms/step\n",
      "100/100 [==============================] - 9s 91ms/step\n",
      "100/100 [==============================] - 9s 91ms/step\n",
      "100/100 [==============================] - 9s 92ms/step\n",
      "100/100 [==============================] - 9s 91ms/step\n",
      "100/100 [==============================] - 9s 91ms/step\n",
      "100/100 [==============================] - 9s 91ms/step\n",
      "100/100 [==============================] - 9s 92ms/step\n",
      "100/100 [==============================] - 9s 90ms/step\n",
      "100/100 [==============================] - 9s 90ms/step\n",
      "100/100 [==============================] - 9s 90ms/step\n",
      "100/100 [==============================] - 9s 90ms/step\n",
      "100/100 [==============================] - 9s 90ms/step\n",
      "100/100 [==============================] - 9s 90ms/step\n",
      "100/100 [==============================] - 9s 89ms/step\n",
      "98/98 [==============================] - 9s 89ms/step\n"
     ]
    }
   ],
   "source": [
    "#ensemble multiple model\n",
    "ensemble = np.empty((3698, 320, 480, 4), dtype=np.float16)\n",
    "for i in range(len(all_models)):\n",
    "    print('running: {}'.format(all_models[i]))\n",
    "    model = None\n",
    "    model = load_model(all_models[i],\n",
    "                       custom_objects={'bce_dice_loss': bce_dice_loss, 'dice_coef': dice_coef})\n",
    "    test_df = []\n",
    "\n",
    "    for i in range(0, test_imgs.shape[0], 100):\n",
    "        batch_idx = list(\n",
    "            range(i, min(test_imgs.shape[0], i + 100))\n",
    "        )\n",
    "\n",
    "        test_generator = DataGenerator(\n",
    "            batch_idx,\n",
    "            df=test_imgs,\n",
    "            shuffle=False,\n",
    "            mode='predict',\n",
    "            input_size=(350, 525),\n",
    "            reshape=(320, 480),\n",
    "            #gamma=0.8,\n",
    "            channels=3,\n",
    "            base_path=TEST_IMAGE,\n",
    "            target_df=sub_df,\n",
    "            batch_size=1,\n",
    "            classes=4\n",
    "        )\n",
    "\n",
    "        batch_pred_masks = model.predict_generator(\n",
    "            test_generator, \n",
    "            workers=1,\n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "        ensemble[i:i+len(batch_idx), ] = np.add(ensemble[i:i+len(batch_idx), ], batch_pred_masks*0.25)\n",
    "    \n",
    "    keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3698, 320, 480, 4)\n"
     ]
    }
   ],
   "source": [
    "#save ensemble model's predictions to npy\n",
    "print(ensemble.shape)\n",
    "np.save(\"4fold_ensemble\", ensemble)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict and Submission (w/o post-process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict on test sets\n",
    "test_df = []\n",
    "for i in range(0, test_imgs.shape[0], 500):\n",
    "    batch_idx = list(\n",
    "        range(i, min(test_imgs.shape[0], i + 500))\n",
    "    )\n",
    "\n",
    "    for j, idx in enumerate(batch_idx):\n",
    "        filename = test_imgs['Image'].iloc[idx]\n",
    "        image_df = sub_df[sub_df['Image'] == filename].copy()\n",
    "        pred_masks = ensemble[i+j, ].round().astype(int)\n",
    "\n",
    "        pred_rles = build_rles(pred_masks, reshape=(350, 525))\n",
    "\n",
    "        image_df['EncodedPixels'] = pred_rles\n",
    "        test_df.append(image_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save predictions for submission\n",
    "test_df = pd.concat(test_df)\n",
    "pre_df = test_df.copy()\n",
    "test_df.drop(columns='Image', inplace=True)\n",
    "test_df.to_csv('submission_v40_ensemble_v2.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict and Submission with Post-process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set threshold and minsize mask to each cloud category\n",
    "thresholds = [0.6, 0.40, 0.55, 0.40]\n",
    "minsizes = [10000, 15000, 10000, 15000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = []\n",
    "for i in range(0, test_imgs.shape[0], 500):\n",
    "    batch_idx = list(\n",
    "        range(i, min(test_imgs.shape[0], i + 500))\n",
    "    )\n",
    "\n",
    "    for j, idx in enumerate(batch_idx):\n",
    "        filename = test_imgs['Image'].iloc[idx]\n",
    "        image_df = sub_df[sub_df['Image'] == filename].copy()\n",
    "        #img = cv2.imread(f'{TEST_IMAGE}/{filename}')\n",
    "        #img = np_resize(img, (320, 480))\n",
    "        \n",
    "        pred_masks = np.empty((320, 480, 4))\n",
    "        for n in range(4):  \n",
    "            pred_mask, _, _ = post_process(ensemble[i+j,:,:,n].astype(float) , threshold=thresholds[n],\n",
    "                                           min_size=minsizes[n],\n",
    "                                           input_size=(320, 480),\n",
    "                                           rectangle=False)\n",
    "            pred_masks[:,:,n] = pred_mask \n",
    "            \n",
    "        pred_rles = build_rles(pred_masks, reshape=(350, 525))\n",
    "\n",
    "        image_df['EncodedPixels'] = pred_rles\n",
    "        test_df.append(image_df)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.concat(test_df)\n",
    "pre_df = test_df.copy()\n",
    "test_df.drop(columns='Image', inplace=True)\n",
    "test_df.to_csv('submission_v42_ensemble_v2_posted_v11.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post-Process parameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_idx, val_idx = train_test_split(mask_count_df.index, random_state=2019, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running: model_DenseNet169_fpn_n2.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1107 00:37:05.168400  5064 deprecation.py:323] From C:\\Users\\zessi\\AppData\\Local\\anaconda3\\envs\\tf-20-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 24s 238ms/step\n",
      "100/100 [==============================] - 12s 120ms/step\n",
      "100/100 [==============================] - 12s 116ms/step\n",
      "100/100 [==============================] - 12s 117ms/step\n",
      "100/100 [==============================] - 12s 120ms/step\n",
      "100/100 [==============================] - 12s 116ms/step\n",
      "100/100 [==============================] - 14s 141ms/step\n",
      "100/100 [==============================] - 12s 117ms/step\n",
      "100/100 [==============================] - 12s 117ms/step\n",
      "100/100 [==============================] - 12s 118ms/step\n",
      "100/100 [==============================] - 12s 117ms/step\n",
      "7/7 [==============================] - 1s 125ms/step\n",
      "running: model_resnet101_unet_sm.h5\n",
      "100/100 [==============================] - 18s 179ms/step\n",
      "100/100 [==============================] - 11s 114ms/step\n",
      "100/100 [==============================] - 11s 111ms/step\n",
      "100/100 [==============================] - 11s 112ms/step\n",
      "100/100 [==============================] - 11s 113ms/step\n",
      "100/100 [==============================] - 11s 114ms/step\n",
      "100/100 [==============================] - 11s 112ms/step\n",
      "100/100 [==============================] - 11s 112ms/step\n",
      "100/100 [==============================] - 11s 113ms/step\n",
      "100/100 [==============================] - 11s 112ms/step\n",
      "100/100 [==============================] - 12s 115ms/step\n",
      "7/7 [==============================] - 1s 119ms/step\n",
      "running: model_incepv3_fpn_n2.h5\n",
      "100/100 [==============================] - 18s 180ms/step\n",
      "100/100 [==============================] - 11s 115ms/step\n",
      "100/100 [==============================] - 11s 112ms/step\n",
      "100/100 [==============================] - 12s 116ms/step\n",
      "100/100 [==============================] - 11s 115ms/step\n",
      "100/100 [==============================] - 12s 115ms/step\n",
      "100/100 [==============================] - 11s 114ms/step\n",
      "100/100 [==============================] - 12s 116ms/step\n",
      "100/100 [==============================] - 12s 123ms/step\n",
      "100/100 [==============================] - 11s 114ms/step\n",
      "100/100 [==============================] - 12s 117ms/step\n",
      "7/7 [==============================] - 1s 128ms/step\n",
      "running: model_res34_sm.h5\n",
      "100/100 [==============================] - 11s 108ms/step\n",
      "100/100 [==============================] - 10s 95ms/step\n",
      "100/100 [==============================] - 9s 94ms/step\n",
      "100/100 [==============================] - 9s 95ms/step\n",
      "100/100 [==============================] - 9s 94ms/step\n",
      "100/100 [==============================] - 9s 95ms/step\n",
      "100/100 [==============================] - 9s 94ms/step\n",
      "100/100 [==============================] - 9s 95ms/step\n",
      "100/100 [==============================] - 10s 95ms/step\n",
      "100/100 [==============================] - 9s 94ms/step\n",
      "100/100 [==============================] - 9s 95ms/step\n",
      "7/7 [==============================] - 1s 127ms/step\n"
     ]
    }
   ],
   "source": [
    "#Use multiple model to predict validation sets \n",
    "val_ensemble_pred_masks = np.empty((1107, 320, 480, 4))\n",
    "for i in range(len(all_models)):\n",
    "    print('running: {}'.format(all_models[i]))\n",
    "    model = None\n",
    "    model = load_model(all_models[i],\n",
    "                       custom_objects={'bce_dice_loss': bce_dice_loss, 'dice_coef': dice_coef})\n",
    "    test_df = []\n",
    "    for i in range(0, val_idx.shape[0], 100):\n",
    "        batch = list(\n",
    "            range(i, min(val_idx.shape[0], i + 100))\n",
    "        )\n",
    "        batch_val_idx = val_idx[i:i+len(batch)]\n",
    "\n",
    "        optimal_generator = DataGenerator(batch_val_idx,\n",
    "                                  df=mask_count_df,\n",
    "                                  target_df=train_df,\n",
    "                                  batch_size=1,\n",
    "                                  mode='predict',\n",
    "                                  shuffle=False,\n",
    "                                  augment=False,\n",
    "                                  #gamma=0.8,\n",
    "                                  reshape=(320, 480)\n",
    "                                 )\n",
    "\n",
    "        val_pred_masks = model.predict_generator(\n",
    "            optimal_generator,\n",
    "            workers=1,\n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "        val_ensemble_pred_masks[i:i+len(batch), ] = np.add(val_ensemble_pred_masks[i:i+len(batch), ], val_pred_masks/3)\n",
    "    \n",
    "    keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_dice_coef(y_true, y_pred, smooth=1):\n",
    "    y_true_f = y_true.flatten()\n",
    "    y_pred_f = y_pred.flatten()\n",
    "    intersection = np.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection + smooth) / (np.sum(y_true_f) + np.sum(y_pred_f) + smooth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1107/1107 [01:34<00:00, 11.70it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "#img = np.empty((len(val_idx), 320, 480, 3))\n",
    "true_y = np.empty((len(val_idx),320, 480, 4))\n",
    "#Generator img and true masks\n",
    "for i in tqdm(range(len(val_idx))):           \n",
    "    fn = mask_count_df['Image'].iloc[val_idx[i]]\n",
    "   # image = cv2.imread(f'{TRAIN_IMAGE}/{fn}')\n",
    "    #img[i, ] = np_resize(image, (320, 480))\n",
    "    \n",
    "    rles_df = train_df[train_df['Image'] == fn]\n",
    "    rles = rles_df['EncodedPixels']\n",
    "    true_y[i, ] = mask_maker(rles, input_shape=(1400, 2100), reshape=(320, 480))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid search "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 4/4 [1:04:01<00:00, 974.03s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "cate_label = {0: 'Fish', 1: 'Flower', 2: 'Gravel', 3: 'Sugar'}\n",
    "val_dice = []\n",
    "\n",
    "for c in tqdm(range(4)):\n",
    "    for t in range(30, 80, 5):\n",
    "        t = t/100\n",
    "        for mi in [5000, 10000, 15000, 20000]:\n",
    "            pred_y = np.empty((320, 480))\n",
    "            coef = []\n",
    "            for i in range(len(val_idx)):           \n",
    "                pred_y, _, _ = post_process(val_ensemble_pred_masks[i,:,:,c], threshold=t,\n",
    "                                            min_size=mi, input_size=(320, 480),\n",
    "                                            #image=img[i, ],\n",
    "                                            rectangle=False)              \n",
    "                coef.append(val_dice_coef(true_y[i, :, :, c], pred_y))\n",
    "            coef = np.mean(coef)\n",
    "            val_dice.append([t, mi, coef, cate_label[c]])\n",
    "            #print('{}, {}, {}, ceof = {}'.format(c, t, mi, coef))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    threshold  min_size  dice_coef  cate\n",
      "23       0.55     20000   0.636998  Fish\n",
      "39       0.75     20000   0.633754  Fish\n",
      "27       0.60     20000   0.633318  Fish\n",
      "35       0.70     20000   0.631514  Fish\n",
      "34       0.70     15000   0.631149  Fish\n",
      "    threshold  min_size  dice_coef    cate\n",
      "74       0.70     15000   0.774896  Flower\n",
      "78       0.75     15000   0.774379  Flower\n",
      "62       0.55     15000   0.773696  Flower\n",
      "66       0.60     15000   0.772655  Flower\n",
      "58       0.50     15000   0.772501  Flower\n",
      "     threshold  min_size  dice_coef    cate\n",
      "114       0.70     15000   0.636006  Gravel\n",
      "107       0.60     20000   0.634482  Gravel\n",
      "111       0.65     20000   0.633479  Gravel\n",
      "118       0.75     15000   0.632979  Gravel\n",
      "99        0.50     20000   0.630960  Gravel\n",
      "     threshold  min_size  dice_coef   cate\n",
      "141       0.55     10000   0.610706  Sugar\n",
      "145       0.60     10000   0.608452  Sugar\n",
      "153       0.70     10000   0.606286  Sugar\n",
      "149       0.65     10000   0.606216  Sugar\n",
      "134       0.45     15000   0.605039  Sugar\n"
     ]
    }
   ],
   "source": [
    "opt_df = pd.DataFrame(val_dice, columns=['threshold', 'min_size', 'dice_coef', 'cate'])\n",
    "opt_df.to_csv('parameter_optimal_ensembl_v2.csv', index=False)\n",
    "\n",
    "for i in range(4):\n",
    "    print(opt_df[opt_df['cate'] == cate_label[i]].sort_values(by='dice_coef', ascending=False).head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict and Submission with Post-process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set optimal threshold and minsize mask to each cloud category\n",
    "thresholds = [0.55, 0.70, 0.70, 0.55]\n",
    "minsizes = [20000, 15000, 150000, 10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = []\n",
    "for i in range(0, test_imgs.shape[0], 500):\n",
    "    batch_idx = list(\n",
    "        range(i, min(test_imgs.shape[0], i + 500))\n",
    "    )\n",
    "\n",
    "    for j, idx in enumerate(batch_idx):\n",
    "        filename = test_imgs['Image'].iloc[idx]\n",
    "        image_df = sub_df[sub_df['Image'] == filename].copy()\n",
    "        #img = cv2.imread(f'{TEST_IMAGE}/{filename}')\n",
    "        #img = np_resize(img, (320, 480))\n",
    "        \n",
    "        pred_masks = np.empty((320, 480, 4))\n",
    "        for n in range(4):  \n",
    "            pred_mask, _, _ = post_process(ensemble[i+j,:,:,n].astype(float) , threshold=thresholds[n],\n",
    "                                           min_size=minsizes[n],\n",
    "                                           input_size=(320, 480),\n",
    "                                           rectangle=False)\n",
    "            pred_masks[:,:,n] = pred_mask \n",
    "            \n",
    "        pred_rles = build_rles(pred_masks, reshape=(350, 525))\n",
    "\n",
    "        image_df['EncodedPixels'] = pred_rles\n",
    "        test_df.append(image_df)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.concat(test_df)\n",
    "pre_df = test_df.copy()\n",
    "test_df.drop(columns='Image', inplace=True)\n",
    "test_df.to_csv('submission_v42_ensemble_v2_posted_v11.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
